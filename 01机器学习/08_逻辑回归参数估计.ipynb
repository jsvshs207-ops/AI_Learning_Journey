{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4.2 逻辑回归的参数估计：极大似然估计，相对熵与交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型基本结构构建完成之后，接下来我们开始讨论如何进行逻辑回归的参数估计。所谓参数估计，其实就是模型参数求解的更加具有统计风格的称呼。根据逻辑回归的基本公式：\n",
    "\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-\\left(\\hat{w}^T \\cdot \\hat{x}\\right)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难看出，逻辑回归的参数其实就是线性方程中的自变量系数和截距。不过由于加入了联系函数，逻辑回归的参数并不能像线性回归一样利用最小二乘法进行快速求解。\n",
    "\n",
    "当然，和所有机器学习模型一样，要求解模型参数，就必须构造损失函数，然后根据损失函数的基本情况寻找优化算法求解。对于逻辑回归来说，课上将介绍两种不同的方法来创建和求解损失函数，两种方法出发点各不相同但却殊途同归：分别是极大似然估计和通过相对熵构建交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 画图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 自定义模块\n",
    "from ML_basic_function import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、逻辑回归参数估计基本思路\n",
    "\n",
    "比较有趣的一点是，尽管逻辑回归的损失函数构建过程比较复杂，但逻辑回归的损失函数的基本形式比较容易理解。因此首先我们先通过一个简单的例子来讨论关于逻辑回归的参数估计的基本思路，即损失函数构建和求解的一般思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 构建损失函数\n",
    "\n",
    "现有简单数据集如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| sepal_length | species |\n",
    "|---|:---|\n",
    "| 1 | 0 |\n",
    "| 3 | 1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于只有一个特征，因此可以构建逻辑回归模型为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y=\\operatorname{sigmoid}(w x+b)=\\frac{1}{1+e^{-(w x+b)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将模型输出结果视作概率，则分别带入两条数据可得模型输出结果为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& p(y=1 \\mid x=1)=\\frac{1}{1+e^{-(w+b)}} \\\\\n",
    "& p(y=1 \\mid x=3)=\\frac{1}{1+e^{-(3 w+b)}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中p(y=1 | x=1)表示x取值为1时y取值为1的条件概率。而我们知道，两条数据的真实情况为第一条数据y取值为0，而第二条数据y取值为1，因此我们可以计算p(y=0 | x=1)如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(y=0 \\mid x=1)=1-p(y=1 \\mid x=1)=1-\\frac{1}{1+e^{-(w+b)}}=\\frac{e^{-(w+b)}}{1+e^{-(w+b)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，损失函数的构建目标和模型评估指标保持一致(例如SSELoss和SSE),对于大多数分类模型来说，模型预测的准确率都是最基础的评估指标。此处如果我们希望模型预测结果尽可能准确，就等价于希望p(y=0 | x=1)和p(y=1 | x=3)两个概率结果越大越好。该目标可以统一在求下式最大值的过程中："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(y=0 \\mid x=1) \\cdot p(y=1 \\mid x=3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即我们希望x取1时y取0，和x取3时y取1的同时发生的概率越大越好。\n",
    "\n",
    "此外，考虑到损失函数一般都是求最小值，因此可将上式求最大值转化为对应负数结果求最小值，同时累乘也可以转化为对数相加结果，因此上式求最大值可等价于下式求最小值："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Logit} \\operatorname{Loss}(w, b) & =-\\ln (p(y=1 \\mid x=3))-\\ln (p(y=0 \\mid x=1)) \\\\\n",
    "& =-\\ln \\left(\\frac{1}{1+e^{-(3 w+b)}}\\right)-\\ln \\left(\\frac{e^{-(w+b)}}{1+e^{-(w+b)}}\\right) \\\\\n",
    "& =\\ln \\left(1+e^{-(3 w+b)}\\right)+\\ln \\left(1+\\frac{1}{e^{-(w+b)}}\\right) \\\\\n",
    "& =\\ln \\left(1+e^{-(3 w+b)}+e^{(w+b)}+e^{-2 w}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们即构建了一个由两条数据所构成的逻辑回归损失函数。\n",
    "\n",
    "> 回顾此前课程内容：损失函数和带入数据量息息相关。\n",
    "\n",
    "注意，在上述损失函数的构建过程中有两个关键步骤，需要再次提醒。\n",
    "\n",
    "其一时将模型高准确率的诉求具象化为$ p (y=0 | x=1)·p(y=1 | x=3)$参数的过程，此处我们为何不能采用类似SSE的计算思路去构建损失函数，即进行如下运算：\n",
    "\n",
    "$$\n",
    "\\|y-y h a t\\|_2^2=\\left\\|y-\\frac{1}{1+e^{-(\\hat{\\hat{w}} \\cdot \\vec{x})}}\\right\\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一般不会采用该方法构建损失函数，其根本原因在于，在数学层面上我们可以证明，对于逻辑回归，当y属于0-1分类变量时，$\\|y-y h a t\\|_2^2$ 损失函数并不是凸函数，而非凸的损失函数将对后续参数最优求解造成很大麻烦。而相比之下，概率连乘所构建的损失函数是凸函数，可以快速求解出全域最小值。\n",
    "\n",
    "其二，在构建损失函数的过程中，我们需要将概率连乘改为对数累加，有一个很重要的原因是，在实际建模运算过程中，尤其是面对大量数据进行损失函数构建过程中，由于有多少条数据就要进行多少次累乘，而累乘的因子又是介于（0，1）之间的数，因此极有可能累乘得到一个非常小的数字，而通用的计算框架计算精度有限，极有可能在累乘过程中损失大量精度，而转化为对数累加之后能很好的避免该问题的发生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.损失函数求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数学角度可以证明，按照上述构成构建的逻辑回归损失函数仍然是凸函数，此时我们仍然可以通过对LogitLos5s(w,b)求偏导然后令偏导函数等于0、再联立方程组的方式来对参数进行求解。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial \\operatorname{Logit} \\operatorname{Loss}(w, b)}{\\partial w}=0 \\\\\n",
    "& \\frac{\\partial \\operatorname{Logit} \\operatorname{Loss}(w, b)}{\\partial b}=0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，上述构建损失函数和求解损失函数的过程，也被称为极大似然估计。接下来我们就将极大似然估计的方法推广到一般过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、利用极大似然估计进行参数估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们考虑更为一般的情况，围绕逻辑回归方程的一般形式，采用极大似然估计方法进行参数估计：\n",
    "\n",
    "逻辑回归模型："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y=\\frac{1}{1+e^{-\\left(\\hat{w}^T \\cdot \\hat{x}\\right)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "$$\n",
    "\\hat{w}=\\left[w_1, w_2, \\ldots w_d, b\\right]^T, \\hat{x}=\\left[x_1, x_2, \\ldots x_d, 1\\right]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求解过程总共分为四个步骤，分别是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 1.确定似然项\n",
    "\n",
    "所谓似然函数，可简单理解为前例中累乘的函数。而累乘过程中的每个项，可称为似然项，不难发现，似然项其实和数据是一对应的，带入多少条数据进行建模，似然函数中就有多少个似然项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道，对于逻辑回归来说，当$\\hat{w}$ 和$\\hat{x}$取得一组之后，既可以有一个概率预测输出结果，即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(y=1 \\mid \\hat{x} ; \\hat{w})=\\frac{1}{1+e^{-\\left(\\hat{w}^T \\cdot \\hat{x}\\right)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对应y取0的概率为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "1-p(y=1 \\mid \\hat{x} ; \\hat{w})=1-\\frac{1}{1+e^{-\\left(\\hat{w}^T \\cdot \\hat{x}\\right)}}=\\frac{e^{-\\left(\\hat{w}^T \\cdot \\hat{x}\\right)}}{1+e^{-\\left(\\hat{w}^T \\cdot \\hat{x}\\right)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{gathered}\n",
    "p_1(\\hat{x} ; \\hat{w})=p(y=1 \\mid \\hat{x} ; \\hat{w}) \\\\\n",
    "p_0(\\hat{x} ; \\hat{w})=1-p(y=1 \\mid \\hat{x} ; \\hat{w})\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，第 i 个数据所对应的似然项可以写成："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_1(\\hat{x} ; \\hat{w})^{y_i} \\cdot p_0(\\hat{x} ; \\hat{w})^{\\left(1-y_i\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$y_{i}$表示第i条数据对应的类别标签。不难发现，当$y_{i} = 0$ 时，代表的是第i条数据标签为0，此时需要带入似然函数的似然项是$p_0(\\hat{x} ; \\hat{w})$ （因为希望$p_{0}$ 的概率更大）。反之，当 $y_{i} = 1$ 时，代表的是第i条数据标签为1，此时需要带入似然函数的似然项是 $p_1(\\hat{x} ; \\hat{w})$。上述似然项可以同时满足这两种不同的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 2.构建似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，通过似然项的累乘计算极大似然函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\prod_{i=1}^N\\left[p_1(\\hat{x} ; \\hat{w})^{y_i} \\cdot p_0(\\hat{x} ; \\hat{w})^{\\left(1-y_i\\right)}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 3.进行对数转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后即可在似然函数基础上对其进行（以e为底的）对数转换，为了方便后续利用优化方法求解最小值，同样我们考虑构建负数对数似然函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\hat{w}) & =-\\ln \\left(\\prod_{i=1}^N\\left[p_1(\\hat{x} ; \\hat{w})^{y_i} \\cdot p_0(\\hat{x} ; \\hat{w})^{\\left(1-y_i\\right)}\\right]\\right) \\\\\n",
    "& =\\sum_{i=1}^N\\left[-y_i \\cdot \\ln \\left(p_1(\\hat{x} ; \\hat{w})\\right)-\\left(1-y_i\\right) \\cdot \\ln \\left(p_0(\\hat{x} ; \\hat{w})\\right)\\right] \\\\\n",
    "& =\\sum_{i=1}^N\\left[-y_i \\cdot \\ln \\left(p_1(\\hat{x} ; \\hat{w})\\right)-\\left(1-y_i\\right) \\cdot \\ln \\left(1-p_1(\\hat{x} ; \\hat{w})\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式推导至此即可，后续我们将借助该公式进行损失函数求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4.求解对数似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过一系列数学过程可以证明，通过极大似然估计构建的损失函数是凸函数，此时我们可以采用导数为0联立方程组的方式进行求解，这也是极大似然估计对参数求解的一般方法。但这种方法会涉及大量的导数运算、方程组求解等，并不适用于大规模甚至是超大规模数值运算，因此，在机器学习领域，我们通常会采用一些更加通用的优化方法对逻辑回归的损失函数进行求解，通常来说是牛顿法或者梯度下降算法，其中，梯度下降算法是机器学习中最为通用的求解损失函数的优化算法，我们将在下一小节花费一整节的时间侠进行介绍。本节我们将继续介绍另外一种推导逻辑回归损失函数的方法—KL离散度计算法，并介绍有关信息熵、交叉熵等关键概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 由于模型本身和损失函数构建方式都和线性回归有所不同，逻辑回归的损失函数无法采用最小二乘法进行求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、熵，相对熵与交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们介绍另一种构建逻辑回归损失函数的基本思路————借助相对熵（relative entropy,又称KL离散度）构建损失函数。尽管最终损失函数构建结果和极大似然估计相同，但该过程所涉及到的关于信息熵(entropy)、相对熵等概念却是包括EM算法、决策树算法等诸多机器学习算法的理论基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常我们用熵(entropy)来表示随机变量不确定性的度量，或者说系统混乱程度、信息混乱程度。熵的计算公式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X)=-\\sum_{i=1}^n p\\left(x_i\\right) \\log \\left(p\\left(x_i\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$p(x_{i})$ 表示多分类问题中第i个类别出现的概率，表示类别总数，通常来说信息熵的计算都取底数为2，并且规定 $ log 0 = 0$。举例说明信息熵计算过程，假设有二分类数据集1标签如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集1\n",
    "\n",
    "| index | labels |\n",
    "|---|:---|\n",
    "| 1 | 0 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| 4 | 1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则信息熵的计算过程中n=2,令p(x1)表示类别0的概率，p(x2)表示类别1的概率（反之亦然），则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& p\\left(x_1\\right)=\\frac{1}{4} \\\\\n",
    "& p\\left(x_2\\right)=\\frac{3}{4}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则该数据集的信息熵计算结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "H(X) & =-\\left(p\\left(x_1\\right) \\log \\left(p\\left(x_1\\right)\\right)+p\\left(x_2\\right) \\log \\left(p\\left(x_2\\right)\\right)\\right) \\\\\n",
    "& =-\\left(\\frac{1}{4}\\right) \\log \\left(\\frac{1}{4}\\right)-\\left(\\frac{3}{4}\\right) \\log \\left(\\frac{3}{4}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1/4 * np.log2(1/4) - 3/4 * np.log2(3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们也可以定义信息熵计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单测试函数性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(1/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，在二分类问题中，$ n = 2$且 $p(x_{1}) + p(x_{2}) = 1$，我们也可以推导二分类的信息熵计算公式为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X)=-p(x) \\log (p(x))-(1-p(x)) \\log (1-p(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$p(x)$为样本标签为0或1的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 熵的基本性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以证明，熵的计算结果在[0,1]之间，并且熵值越大，系统越混乱、信息越混乱。例如，有如下两个数据集其中数据集2总共4条样本，0、1类各占50%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于该数据集，我们可以计算信息熵为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_1(X)=-\\left(\\frac{1}{2}\\right) \\log \\left(\\frac{1}{2}\\right)-\\left(\\frac{1}{2}\\right) \\log \\left(\\frac{1}{2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时信息熵达到最高值，也就代表对于上述二分类的数据集，标签随机变量的不确定性已经达到峰值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步我们计算下列数据集3的信息熵："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵计算可得："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_1(X)=-\\left(\\frac{4}{4}\\right) \\log \\left(\\frac{4}{4}\\right)-\\left(\\frac{0}{4}\\right) \\log \\left(\\frac{0}{4}\\right)=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时信息熵取得最小值，也就代表标签的取值整体呈现非常确定的状态，系统信息规整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 值得一提的是，此时标签本身的信息量也为0，并没有进一步进行预测的必要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合上述三个数据集，不难看出，当标签取值不均时信息嫡较高，标签取值纯度较高时信息熵较低。假设为未分类数据集中1样本所占比例，则数据集信息嫡随着变化为变化趋势如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(0, 1, 50)\n",
    "ent_1 = [entropy(p_i) for p_i in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(p, ent_1)\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 相对熵(relative entropy)与交叉熵(cross entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对熵也被称为Kullback-Leibleri散度(KL散度)或者信息散度(information divergence),通常用来衡量两个随机变量分布的差异性。假设对同一个随机变量X,有两个单独的概率分布P(x)和Q(x),当X是离散变量时，我们可以通过如下相对熵计算公式来衡量二者差异："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{K L}(P \\| Q)=\\sum_{i=1}^n P\\left(x_i\\right) \\log \\left(\\frac{P\\left(x_i\\right)}{Q\\left(x_i\\right)}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和信息熵类似，相对熵越小，代表Q(x)和P(x)越接近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从交叉熵的计算公式不难看出，这其实是一种非对称性度量，也就是  $D_{KL}(P || Q) ≠ D_{kL}(Q || P) $。从本质上来说，相对熵刻画的是用概率分布 $ Q $ 来刻画概率分布 $ P $ 的困难程度，而在机器学习领域，我们一般令$ Q $ 为模型输出结果，而 $ P $ 为数据集标签真实结果，以此来判断模型输出结果是否足够接近真实情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q为拟合分布P为真实分布，也被称为前向KL散度(forward KL divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，上述相对熵公式等价于："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "D_{K L}(P \\| Q) & =\\sum_{i=1}^n P\\left(x_i\\right) \\log \\left(\\frac{P\\left(x_i\\right)}{Q\\left(x_i\\right)}\\right) \\\\\n",
    "& =\\sum_{i=1}^n P\\left(x_i\\right) \\log \\left(P\\left(x_i\\right)\\right)-\\sum_{i=1}^n P\\left(x_i\\right) \\log \\left(Q\\left(x_i\\right)\\right) \\\\\n",
    "& =-H(P(x))+\\left[-\\sum_{i=1}^n P\\left(x_i\\right) \\log \\left(Q\\left(x_i\\right)\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对于给定数据集，信息熵 $H（p(X)）$ 是确定的，因此相对熵的大小完全由$-\\sum_{i=1}^n P\\left(x_i\\right) \\log \\left(Q\\left(x_i\\right)\\right)$ 决定。而该式计算结果也被称为交叉熵(cross entropy)计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{cross\\_entropy}(P, Q) = -\\sum_{i=1}^n P(x_i) \\log(Q(x_i))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，如果我们希望P、Q二者分布尽可能接近，我们就需要尽可能减少相对熵，但由于相对熵=交叉熵-信息熵，因此我们只能力求减少交叉熵。当然，也正因如此，交叉嫡可以作为衡量模型输出分布是否接近真实分布的重要度量方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单总结上述过程要点："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我们使用相对熵 $ D_{KL}(P || Q)$ 来表示模型拟合分布Q和数据真实分布P之间的差距，相对熵越小，拟合效果越好；\n",
    "\n",
    "* 根据计算公式，$D_{K L}(P \\| Q)=-H(P(x))+\\left[-\\sum_{i=1}^n P\\left(x_i\\right) \\log \\left(Q\\left(x_i\\right)\\right)\\right]$ ,相对熵= 交叉熵-信息熵；\n",
    "\n",
    "* 对于给定数据集，信息熵是确定的，因此我们只能通过尽可能减小交叉熵来降低相对熵；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 根据吉布斯不等式，相对熵的取值恒大于零，当预测分布和真实分布完全一致时相对熵取值为0，此时交叉熵等于数据信息熵，此外只要二者分布不一致，交叉熵的取值都将大于信息熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 单样本交叉熵计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵的计算公式看似复杂，但实际运算过程比较简单，对于类似逻辑回归模型输出为连续变量，而真实标签为离散变脸的数据集，可以举例说明计算过程。例如有数据集情况如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将其改写成如下形式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-57.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中A、B表示每条样本可能所属的类别。围绕该数据集，第一条数据的交叉熵计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{cross\\_entropy} = -0 \\cdot \\log(0.2) - 1 \\cdot \\log(0.8)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3219280948873623"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log2(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 再次理解交叉熵计算公式中的叠加就是类别的叠加\n",
    "\n",
    "> 上述数据集标签由0-1转化为A,B，也被称为名义型变量的独热编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对于多个数据集，整体交叉熵实际上是每条数据交叉熵的均值。例如上述数据集，整体交叉嫡计算结果为：\n",
    "\n",
    "$$\n",
    "\\frac{-1 * \\log (0.8)-1 * \\log (0.7)-1 * \\log (0.6)-1 * \\log (0.7)}{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5220100086782713"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-np.log2(0.8) - np.log2(0.7) - np.log2(0.6) - np.log2(0.7)) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此，我们可以给出多样本交叉熵计算公式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{cross\\_entropy} = -\\frac{1}{m} \\sum_{j=1}^m \\sum_{i=1}^n p(p_{ij}) \\log(q_{ij})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中m为数据量，n为类别数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 对比极大似然估计函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "围绕上述数据集，如果考虑采用极大似然估计来进行计算，我们发现基本计算流程保持一致："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\hat{w})=\\sum_{i=1}^N\\left[-y_i \\cdot \\ln \\left(p_1(\\hat{x} ; \\hat{w})\\right)-\\left(1-y_i\\right) \\cdot \\ln \\left(1-p_1(\\hat{x} ; \\hat{w})\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带入数据可得："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "-\\ln (0.8)-\\ln (0.7)-\\ln (0.6)-\\ln (0.7)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4473190629576653"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0.8) - np.log(0.7) - np.log(0.6) - np.log(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管具体数值计算结果有所差异，但基本流程都是类似的取类别1的概率的对数运算结果进行累加再取负数。因此在实际建模过程中，考虑采用极大似然估计构建损失函数，和采用交叉熵构建损失函数，效果是相同的，二者构建的损失函数都能很好的描绘模型预测结果和真实结果的差异程度。不过在机器学习领域，一般以交叉嫡损失函数为主。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 二分类交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此，我们也可最终推导二分类交叉熵损失函数计算公式，结合极大似然估计的计算公式和交叉熵的基本计算流程，二分类交叉嫡损失函数为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\operatorname{binary} C E(\\hat{w})=-\\frac{1}{n} \\sum_{i=1}^N\\left[y_i \\cdot \\log \\left(p_1(\\hat{x} ; \\hat{w})\\right)+\\left(1-y_i\\right) \\cdot \\log \\left(1-p_1(\\hat{x} ; \\hat{w})\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以定义一个函数来进行二分类交叉嫡损失函数的计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE(y, yhat):\n",
    "    \"\"\"二分类交叉熵损失函数\n",
    "    y: 标签\n",
    "    yhat: 预测值\n",
    "    \"\"\"\n",
    "    return (-(1/len(y)) * np.sum(y * np.log2(yhat) + (1-y) * np.log2(1-yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单进行验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 0, 1]).reshape(-1, 1)\n",
    "yhat = np.array([0.8, 0.3, 0.4, 0.7]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5220100086782713"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BCE(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们就完成了完整的逻辑回归损失函数的构建。但正如此前所讨论的一样，对于逻辑回归的损失函数来说，尽管也是凸函数，但无法使用最小二乘法进行求解。在下一节，我们将介绍一种更加通用的求解损失函数的优化算法—梯度下降。并最终完成逻辑回归的参数求解。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
