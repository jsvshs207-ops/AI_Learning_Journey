{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6.6.1 关于多分类评估指标的macro与micro过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在正式讨论关于网格搜索的进阶使用方法之前，我们需要先补充一些关于多分类问题的评估指标计算过程。在此前的课程中，我们曾经介绍过分类模型在解决多分类问题时的不同策路，同时也介绍过二分类问题的更高级评估指标，如F1-score和ROC-AUC等，接下来我们将详细讨论关于多分类预测结果在F1-socre和ROC-AUC中的评估过程，以及在sklearn中如何调用函数进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 画图模块\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 机器学习模块\n",
    "from ML_basic_function import *\n",
    "\n",
    "# Scikit-Learn\n",
    "# 评估模器模块\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 数据准备\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 多分类F1-Score评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先导入和F1-Score相关的评估指标计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后简单查看相关说明文档，发现这几组合混淆矩阵相关的评估指标基本是共用了一套参数命名，并且大多数参数其实都是作用于多分类问题，对于二分类问题，我们可以简单调用相关函数直接计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0, 1, 0, 1])\n",
    "y_pred = np.array([1, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75, 1.0, 0.8571428571428571)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true, y_pred), recall_score(y_true, y_pred), f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mzero_division\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Compute the precision.\n",
      "\n",
      "The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "true positives and ``fp`` the number of false positives. The precision is\n",
      "intuitively the ability of the classifier not to label as positive a sample\n",
      "that is negative.\n",
      "\n",
      "The best value is 1 and the worst value is 0.\n",
      "\n",
      "Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "precision for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\n",
      "and precision for both classes are computed, then averaged or both returned (when\n",
      "`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\n",
      "precision for all `labels` are either returned or averaged depending on the\n",
      "`average` parameter. Use `labels` specify the set of labels to calculate precision\n",
      "for.\n",
      "\n",
      "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "    Ground truth (correct) target values.\n",
      "\n",
      "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "    Estimated targets as returned by a classifier.\n",
      "\n",
      "labels : array-like, default=None\n",
      "    The set of labels to include when `average != 'binary'`, and their\n",
      "    order if `average is None`. Labels present in the data can be\n",
      "    excluded, for example in multiclass classification to exclude a \"negative\n",
      "    class\". Labels not present in the data can be included and will be\n",
      "    \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "    By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "\n",
      "    .. versionchanged:: 0.17\n",
      "       Parameter `labels` improved for multiclass problem.\n",
      "\n",
      "pos_label : int, float, bool or str, default=1\n",
      "    The class to report if `average='binary'` and the data is binary,\n",
      "    otherwise this parameter is ignored.\n",
      "    For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "    `average != 'binary'` to report metrics for one label only.\n",
      "\n",
      "average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "    This parameter is required for multiclass/multilabel targets.\n",
      "    If ``None``, the scores for each class are returned. Otherwise, this\n",
      "    determines the type of averaging performed on the data:\n",
      "\n",
      "    ``'binary'``:\n",
      "        Only report results for the class specified by ``pos_label``.\n",
      "        This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "    ``'micro'``:\n",
      "        Calculate metrics globally by counting the total true positives,\n",
      "        false negatives and false positives.\n",
      "    ``'macro'``:\n",
      "        Calculate metrics for each label, and find their unweighted\n",
      "        mean.  This does not take label imbalance into account.\n",
      "    ``'weighted'``:\n",
      "        Calculate metrics for each label, and find their average weighted\n",
      "        by support (the number of true instances for each label). This\n",
      "        alters 'macro' to account for label imbalance; it can result in an\n",
      "        F-score that is not between precision and recall.\n",
      "    ``'samples'``:\n",
      "        Calculate metrics for each instance, and find their average (only\n",
      "        meaningful for multilabel classification where this differs from\n",
      "        :func:`accuracy_score`).\n",
      "\n",
      "sample_weight : array-like of shape (n_samples,), default=None\n",
      "    Sample weights.\n",
      "\n",
      "zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "    Sets the value to return when there is a zero division.\n",
      "\n",
      "    Notes:\n",
      "    - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "    - If set to `np.nan`, such values will be excluded from the average.\n",
      "\n",
      "    .. versionadded:: 1.3\n",
      "       `np.nan` option was added.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)\n",
      "    Precision of the positive class in binary classification or weighted\n",
      "    average of the precision of each class for the multiclass task.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "    support for each class.\n",
      "recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the\n",
      "    number of true positives and ``fn`` the number of false negatives.\n",
      "PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "    an estimator and some data.\n",
      "PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "    binary class predictions.\n",
      "multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "    sample.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "When ``true positive + false positive == 0``, precision returns 0 and\n",
      "raises ``UndefinedMetricWarning``. This behavior can be\n",
      "modified with ``zero_division``.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> import numpy as np\n",
      ">>> from sklearn.metrics import precision_score\n",
      ">>> y_true = [0, 1, 2, 0, 1, 2]\n",
      ">>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      ">>> precision_score(y_true, y_pred, average='macro')\n",
      "0.22...\n",
      ">>> precision_score(y_true, y_pred, average='micro')\n",
      "0.33...\n",
      ">>> precision_score(y_true, y_pred, average='weighted')\n",
      "0.22...\n",
      ">>> precision_score(y_true, y_pred, average=None)\n",
      "array([0.66..., 0.        , 0.        ])\n",
      ">>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      ">>> precision_score(y_true, y_pred, average=None)\n",
      "array([0.33..., 0.        , 0.        ])\n",
      ">>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "array([0.33..., 1.        , 1.        ])\n",
      ">>> precision_score(y_true, y_pred, average=None, zero_division=np.nan)\n",
      "array([0.33...,        nan,        nan])\n",
      "\n",
      ">>> # multilabel classification\n",
      ">>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      ">>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      ">>> precision_score(y_true, y_pred, average=None)\n",
      "array([0.5, 1. , 1. ])\n",
      "\u001b[1;31mFile:\u001b[0m      e:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "precision_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-118.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，需要重点介绍多分类问题时average参数不同取值时的计算方法。此处以recall为例进行计算，重点介绍当averagel取值为'macro'、'micro和'weighted的情况，其他指标也类似，例如有简单多分类问题如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-119.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们令1类标签为0；2类标签为1；3类标签为2，则上述数据集真实标签为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 2, 2, 0, 1, 1,2, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并且最终分类预测结果为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([0, 1, 0, 2, 2, 1, 2, 2, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此，我们可以构造多分类混淆矩阵如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-120.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此我们可以计算三个类别中的TP和FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp1 =2\n",
    "tp2 = 2\n",
    "tp3 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn1 = 1\n",
    "fn2 = 1\n",
    "fn3 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来有两种计算recall的方法，其一是先计算每个类别的recall，然后求均值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = tp1/(tp1+fn1)\n",
    "re2 = tp2/(tp2+fn2)\n",
    "re3 = tp3/(tp3+fn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6944444444444443"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([re1, re2, re3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这也就是average参数取值为macro时的计算结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6944444444444443"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，如果上述手动实现过程不求均值，而是根据每个类别的数量进行加权求和，则就是参数average参数取值为weighted时的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，还有另外一种计算方法，那就是先计算整体的TP和FN,然后根据整体TP和FN计算recall::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = tp1 + tp2 + tp3\n",
    "fn = fn1 + fn2 + fn3\n",
    "tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上述三个不同参数的选取，首先如果是样本不平衡问题（如果是要侧重训练模型判别小类样本的能力的情况下）、则应排除weighted参数，以避免赋予大类样本更高的权重。除此以外，在大多数情况下这三个不同的参数其实并不会对最后评估器的选取结果造成太大影响，只是在很多要求严谨的场合下需要说明多分类的评估结果的计算过程，此时需要简单标注下是按照何种方法进行的计算。\n",
    "\n",
    "不过，如果是混淆矩阵中相关指标和ROC-AUC指标放在一起讨论，由于新版sklearn中ROC-AUC本身不支持在多分类时按照micro算、只支持macro计算，因此建议混淆矩阵的多分类计算过程也选择macro过程，以保持一致。后续在没有进行其他特殊说明的情况下，课上统一采用macro指标进行多分类问题评估指标的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 不过值得注意的是，还有一种观点，尽管micro和macro方法在混淆矩阵相关指标的计算过程中差别不大，在roc\n",
    "auc中，macro指标并不利于非平衡样本的计算(混淆矩阵中可以通过positive的类别选择来解决这一问题)，需要\n",
    "配合ovr分类方法才能够有所改善。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 多分类ROC-AUC评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来继续讨论关于多分类的ROC-AUC评估指标的相关问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够发现，roc_auc score评估指标函数中大多数参数都和此前介绍的混淆矩阵中评估指标类似。接下来我们简单尝试使用roc-auc函数进行评估指标计算，根据roc-auc的计算流程可知，此处我们需要在y_prd参数位中输入模型概率预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0, 1, 0, 1])\n",
    "y_pred = np.array([0.9, 0.7, 0.2, 0.7, 0.4, 0.8])   # 预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，如果我们在y_pred参数中输入分类结果，该函数也能计算出最终结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0, 1, 0, 1])\n",
    "y_pred = np.array([1, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过，此时模型会默认预测标签为0的概率结果为0.4、预测标签为1的概率预测结果为0.6，即上述结果等价于："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0, 1, 0, 1])\n",
    "y_pred = np.array([0.6, 0.6, 0.4, 0.6, 0.4, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_fpr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\n",
      "\n",
      "Note: this implementation can be used with binary, multiclass and\n",
      "multilabel classification, but some restrictions apply (see Parameters).\n",
      "\n",
      "Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "    True labels or binary label indicators. The binary and multiclass cases\n",
      "    expect labels with shape (n_samples,) while the multilabel case expects\n",
      "    binary label indicators with shape (n_samples, n_classes).\n",
      "\n",
      "y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "    Target scores.\n",
      "\n",
      "    * In the binary case, it corresponds to an array of shape\n",
      "      `(n_samples,)`. Both probability estimates and non-thresholded\n",
      "      decision values can be provided. The probability estimates correspond\n",
      "      to the **probability of the class with the greater label**,\n",
      "      i.e. `estimator.classes_[1]` and thus\n",
      "      `estimator.predict_proba(X, y)[:, 1]`. The decision values\n",
      "      corresponds to the output of `estimator.decision_function(X, y)`.\n",
      "      See more information in the :ref:`User guide <roc_auc_binary>`;\n",
      "    * In the multiclass case, it corresponds to an array of shape\n",
      "      `(n_samples, n_classes)` of probability estimates provided by the\n",
      "      `predict_proba` method. The probability estimates **must**\n",
      "      sum to 1 across the possible classes. In addition, the order of the\n",
      "      class scores must correspond to the order of ``labels``,\n",
      "      if provided, or else to the numerical or lexicographical order of\n",
      "      the labels in ``y_true``. See more information in the\n",
      "      :ref:`User guide <roc_auc_multiclass>`;\n",
      "    * In the multilabel case, it corresponds to an array of shape\n",
      "      `(n_samples, n_classes)`. Probability estimates are provided by the\n",
      "      `predict_proba` method and the non-thresholded decision values by\n",
      "      the `decision_function` method. The probability estimates correspond\n",
      "      to the **probability of the class with the greater label for each\n",
      "      output** of the classifier. See more information in the\n",
      "      :ref:`User guide <roc_auc_multilabel>`.\n",
      "\n",
      "average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n",
      "    If ``None``, the scores for each class are returned.\n",
      "    Otherwise, this determines the type of averaging performed on the data.\n",
      "    Note: multiclass ROC AUC currently only handles the 'macro' and\n",
      "    'weighted' averages. For multiclass targets, `average=None` is only\n",
      "    implemented for `multi_class='ovr'` and `average='micro'` is only\n",
      "    implemented for `multi_class='ovr'`.\n",
      "\n",
      "    ``'micro'``:\n",
      "        Calculate metrics globally by considering each element of the label\n",
      "        indicator matrix as a label.\n",
      "    ``'macro'``:\n",
      "        Calculate metrics for each label, and find their unweighted\n",
      "        mean.  This does not take label imbalance into account.\n",
      "    ``'weighted'``:\n",
      "        Calculate metrics for each label, and find their average, weighted\n",
      "        by support (the number of true instances for each label).\n",
      "    ``'samples'``:\n",
      "        Calculate metrics for each instance, and find their average.\n",
      "\n",
      "    Will be ignored when ``y_true`` is binary.\n",
      "\n",
      "sample_weight : array-like of shape (n_samples,), default=None\n",
      "    Sample weights.\n",
      "\n",
      "max_fpr : float > 0 and <= 1, default=None\n",
      "    If not ``None``, the standardized partial AUC [2]_ over the range\n",
      "    [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n",
      "    should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n",
      "    computation currently is not supported for multiclass.\n",
      "\n",
      "multi_class : {'raise', 'ovr', 'ovo'}, default='raise'\n",
      "    Only used for multiclass targets. Determines the type of configuration\n",
      "    to use. The default value raises an error, so either\n",
      "    ``'ovr'`` or ``'ovo'`` must be passed explicitly.\n",
      "\n",
      "    ``'ovr'``:\n",
      "        Stands for One-vs-rest. Computes the AUC of each class\n",
      "        against the rest [3]_ [4]_. This\n",
      "        treats the multiclass case in the same way as the multilabel case.\n",
      "        Sensitive to class imbalance even when ``average == 'macro'``,\n",
      "        because class imbalance affects the composition of each of the\n",
      "        'rest' groupings.\n",
      "    ``'ovo'``:\n",
      "        Stands for One-vs-one. Computes the average AUC of all\n",
      "        possible pairwise combinations of classes [5]_.\n",
      "        Insensitive to class imbalance when\n",
      "        ``average == 'macro'``.\n",
      "\n",
      "labels : array-like of shape (n_classes,), default=None\n",
      "    Only used for multiclass targets. List of labels that index the\n",
      "    classes in ``y_score``. If ``None``, the numerical or lexicographical\n",
      "    order of the labels in ``y_true`` is used.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "auc : float\n",
      "    Area Under the Curve score.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "average_precision_score : Area under the precision-recall curve.\n",
      "roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "    (ROC) curve given an estimator and some data.\n",
      "RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "    (ROC) curve given the true and predicted values.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "\n",
      ".. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n",
      "        <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
      "\n",
      ".. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n",
      "       probability estimation trees (Section 6.2), CeDER Working Paper\n",
      "       #IS-00-04, Stern School of Business, New York University.\n",
      "\n",
      ".. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n",
      "        Recognition Letters, 27(8), 861-874.\n",
      "        <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n",
      "\n",
      ".. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n",
      "        Under the ROC Curve for Multiple Class Classification Problems.\n",
      "        Machine Learning, 45(2), 171-186.\n",
      "        <http://link.springer.com/article/10.1023/A:1010920819831>`_\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Binary case:\n",
      "\n",
      ">>> from sklearn.datasets import load_breast_cancer\n",
      ">>> from sklearn.linear_model import LogisticRegression\n",
      ">>> from sklearn.metrics import roc_auc_score\n",
      ">>> X, y = load_breast_cancer(return_X_y=True)\n",
      ">>> clf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)\n",
      ">>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
      "0.99...\n",
      ">>> roc_auc_score(y, clf.decision_function(X))\n",
      "0.99...\n",
      "\n",
      "Multiclass case:\n",
      "\n",
      ">>> from sklearn.datasets import load_iris\n",
      ">>> X, y = load_iris(return_X_y=True)\n",
      ">>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
      ">>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
      "0.99...\n",
      "\n",
      "Multilabel case:\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> from sklearn.datasets import make_multilabel_classification\n",
      ">>> from sklearn.multioutput import MultiOutputClassifier\n",
      ">>> X, y = make_multilabel_classification(random_state=0)\n",
      ">>> clf = MultiOutputClassifier(clf).fit(X, y)\n",
      ">>> # get a list of n_output containing probability arrays of shape\n",
      ">>> # (n_samples, n_classes)\n",
      ">>> y_pred = clf.predict_proba(X)\n",
      ">>> # extract the positive columns for each output\n",
      ">>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])\n",
      ">>> roc_auc_score(y, y_pred, average=None)\n",
      "array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\n",
      ">>> from sklearn.linear_model import RidgeClassifierCV\n",
      ">>> clf = RidgeClassifierCV().fit(X, y)\n",
      ">>> roc_auc_score(y, clf.decision_function(X), average=None)\n",
      "array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\n",
      "\u001b[1;31mFile:\u001b[0m      e:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "roc_auc_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-121.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处需要注意的是关于multi_class参数的选择。一般来说sklearn中的multi_class参数都是二分类器中用于解决多元分类问题时的参数（如逻辑回归），而由于roc-auc需要分类结果中的概率来完成最终计算，因此需要知道概率结果对应分类标签一即到底是以ovo还是ovr模式在进行多分类，因此如果是进行多分类roc-auc计算时，需要对其进行明确说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过对于多分类逻辑回归来说，无论是ovr还是mvm策略，最终分类结果其实都可以看成是ovr分类结果，因此如果是多分类逻辑回归计算roc-auc,需要设置multi_class参数为ovr。同时由于根据roc-auc的函数参数说明可知，在muti_class参数取为ovr时，average参数取值为macro时能够保持一个较高的偏态样本敏感性，因此对于roc-auc来说，大多数时候average参数建议取值为macro。\n",
    "总结一下，对于roc-auc进行多分类问题评估时，建议选择的参数组合是ovr/ovo+macro,而ovr/ovo的参数选择需要根据具体的多分类模型来定，如果是围绕逻辑回归多分类评估器来进行结果评估，则建议roc-auc和逻辑回归评估器的multi_class参数都选择ovr。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
