{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 画图模块\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 机器学习模块\n",
    "from ML_basic_function import *\n",
    "\n",
    "# Scikit-Learn\n",
    "# 评估模器模块\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 数据准备\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、集成学习的三大关键领域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习(Ensemblelearning)是机器学习中最先进、最有效、最具研究价值的领域之一，这类方法会训练多个弱评估器(baseestimators)、并将它们输出的结果以某种方式结合起来解决一个问题。\n",
    "\n",
    "在过去十年中，人工智能相关产业蓬勃发展，计算机视觉、自然语言处理、语音识别等领域不断推陈出新、硕果累累，但热闹是深度学习的，机器学习好似什么也没有。2012年之后，传统机器学习占据的搜素、推荐、翻译、各类预测领域都被深度学习替代或入侵，在招聘岗位中，69%的岗位明确要求深度学习技能，传统机器学习算法在这一场轰轰烈烈的人工智能热潮当中似乎有些被冷落了。\n",
    "\n",
    "在人工智能大热的背后，集成学习就如同裂缝中的一道阳光，凭借其先进的思想、优异的性能杀出了一条血路，成为当代机器学习领域中最受学术界和产业界青的领域。\n",
    "\n",
    "从今天的眼光来看，集成学习是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当代工业应用中，唯一能与深度学习算法分庭抗礼的算法\n",
    "  \n",
    "* 数据竞赛高分榜统治者，KDDcup、Kaggle、天池、DC冠军队御用算法\n",
    "\n",
    "* 在搜素、推荐、广告等众多领域，事实上的工业标准和基准模型\n",
    "\n",
    "* 任何机器学习/深度学习工作者都必须掌握其原理、熟读其思想的领域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在集成学习的发展历程中，集成的思想以及方法启发了众多深度学习和机器学习方面的工作，在学术界和工业界都取得了巨大的成功。今天，集成学习可以被分为三个主要研究领域："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **模型融合**\n",
    "\n",
    "模型融合在最初的时候被称为\"分类器结合\"，这个领域主要关注强评估器，试图设计出强大的规则来融合强分类器的结果、以获取更好的融合结果。这个领域的手段主要包括了投票法Voting、堆叠法Stacking、混合法Blending等，且被融合的模型需要是强分类器。模型融合技巧是机器学习/深度学习竞赛中最为可靠的提分手段之一，常言道：当你做了一切尝试都无效，试试模型融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **弱分类器集成**\n",
    "\n",
    "弱分类器集成主要专注于对传统机器学习算法的集成，这个领域覆盖了大部分我们熟悉的集成算法和集成手段，如装袋法bagging，提升法boosting。这个领域试图设计强大的集成算法、来将多个弱学习器提升成为强学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **混合专家模型(mixtureofexperts)**\n",
    "\n",
    "混合专家模型常常出现在深度学习（神经网络）的领域。在其他集成领域当中，不同的学习器是针对同任务、甚至在同一数据上进行训练，但在混合专家模型中，我们将一个复杂的任务拆解成几个相对简单且更小的子任务，然后针对不同的子任务训练个体学习器（专家），然后再结合这些个体学习器的结果得出最终的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在课程当中，我们将对弱分类器集成与模型融合两部分进行详细的说明，混合专家模型我们将会在案例中有专门的讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、 Bagging方法的基本思想 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging又称为\"装袋法\"，它是所有集成学习方法当中最为著名、最为简单、也最为有效的操作之一。\n",
    "\n",
    "在Bagging集成当中，我们并行建立多个弱评估器(通常是决策树，也可以是其他非线性算法)，并综合多个弱评估器的结果进行输出。当集成算法目标是回归任务时，集成算法的输出结果是弱评估器输出的结果的平均值，当集成算法的目标是分类任务时，集成算法的输出结果是弱评估器输出的结果少数服从多数。\n",
    "\n",
    "举例来说，假设现在一个bagging集成算法当中有7个弱评估器，对任意一个样本而言，输出的结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 分类的情况：输出7个弱评估器上的分类结果(0,1,2)\n",
    "r_clf = np.array([0, 2, 1, 1, 2, 1, 0])\n",
    "\n",
    "b_result_clf = np.argmax(np.bincount(r_clf))\n",
    "\n",
    "print(b_result_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bincount会先将array由小到大进行排序，然后对每个数值进行计数，并返回计数结果的函数。需要注意的是，bincount函数不能接受负数输入。\n",
    "\n",
    "* argmax是找到array中最大值，并返回最大值索引的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 如果评估器的数量是偶数，而少数和多数刚好一致怎么办？\n",
    "> * 从数量一致的类别中随机返回一个类别（需要进行随机设置）\n",
    "> * 从数量一致的类别中，返回编码数字更小的类别(如果使用argmax函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9183714285714285"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 回归的情况：输出7个弱评估器上的回归结果\n",
    "r_reg = np.array([-2.082,-0.601,-1.686,-1.001,-2.037, 0.1284, 0.8500])\n",
    "b_result_reg = np.mean(r_reg)\n",
    "b_result_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn当中，我们可以接触到两个Bagging集成算法，一个是随机森林(RandomForest)，另一个是极端随机树(ExtraTrees)，他们都是以决策树为弱评估器的有监督算法，可以被用于分类、回归、排序等各种任务。同时，我们还可以使用bagging的思路对其他算法进行集成，比如使用装袋法分类的类BaggingClassifier对支持向量机或逻辑回归进行集成。在课程当中，我们将重点介绍随机森林的原理与用法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-142.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三, 随机森林RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林是机器学习领域最常用的算法之一，其算法构筑过程非常简单：从提供的数据中随机抽样出不同的子集，用于建立多棵不同的决策树，并按照Bagging的规则对单棵决策树的结果进行集成（回归则平均，分类则少数服从多数）。只要你充分掌握了决策树的各项属性和参数，随机森林的大部分内容都相当容易理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然原理上很简单，但随机森林的学习能力异常强大、算法复杂度高、又具备一定的抗过拟合能力，是从根本上来说比单棵决策树更优越的算法。即便在深入了解机器学习的各种技巧之后，它依然是我们能够使用的最强大的算法之一。原理如此简单、还如此强大的算法在机器学习的世界中是不常见的。在机器学习竞赛当中，随机森林往往是我们在中小型数据上会尝试的第一个算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn中，随机森林可以实现回归也可以实现分类。随机森林回归器由类sklearn.ensemble.RandomForestRegressor实现，随机森林分类器则有类sklearn.ensemble.RandomForestclassifier实现。我们可以像调用逻辑回归、决策树等其他sklearn中的算法一样，使用\"实例化、fit、predict/score\"三部曲来使用随机森林，同时我们也可以使用sklearn中的交叉验证方法来实现随机森林。其中回归森林的默认评估指标为R2，分类森林的默认评估指标为准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 RandomForestRegressor的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import  RandomForestRegressor as RFR\n",
    "from sklearn.tree import  DecisionTreeRegressor as DTR\n",
    "from sklearn.model_selection import  cross_validate, KFold\n",
    "\n",
    "# 这里我们不再使用cross_val_score，因为我们需要同时记录训练和测试的效果\n",
    "# 决策树本身就是非常容易过拟合的算法，而集成模型的参数量/复杂度很难支持大规模网络搜索\n",
    "# 因此对于随机森林来说，一定要关注算法的过拟合情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在集成学习中，我们衡量回归类算法的指标一般是RMSE（根均方误差），也就是MSE开根号后的结果**.现实数据的标签往往数字巨大、数据量庞杂，MSE作为平方结果会放大现实数据上的误差（例如随机森林结果中得到的，7*10°等结果），因此我们会对平房结果开根号，让回归类算法的评估指标在数值上不要过于弯张。同样的，方差作为平方结果，在现实数据上也会太大，因此如果可以，我们使用标准差进行模型稳定性的衡量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 随机森林回归器的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当填写参数之后，随机森林可以变得更强大。比起经典机器学习算法逻辑回归、岭回归等，随机森林回归器的参数数量较多，因此我们可以将随机森林类的参数分为如下4大类别，其中标注为绿色的是我们从未学过的、只与集成算法相关的参数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-143.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 弱分类器结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在集成算法当中，控制单个弱评估器的结构是一个重要的课题，因为单个弱评估器的复杂度/结果都会影响全局，其中单棵决策树的结构越复杂，集成算法的整体复杂度会更高，计算会更加缓慢、模型也会更加容易过拟合，因此集成算法中的弱评估器也需要被剪枝。随机森林回归器的弱评估器是回归树，因此集成评估器中有大量的参数都与弱评估器回归树中的参数重合："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-144.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 分枝标准与特征重要性\n",
    "\n",
    "> criterion与feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与分类树中的信息摘/基尼系数不同，回归树中的criterion可以选择\"squared_error\"（平方误差)，\"absolute_error\" (绝对误差) 以及\"poisson\" (泊松偏差)。对任意样本 $i$ 而言， $y_i$ 为真实标答， $\\hat{y}_i$ 为预测标签，则各个criterion的表达式为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平方误差: $\\sum\\left(y_i-\\hat{y}_i\\right)^2$\n",
    "\n",
    "绝对误差: $\\sum\\left|y_i-\\hat{y_i}\\right|$\n",
    "\n",
    "泊松偏差: $2 \\sum\\left(y_i \\log \\left(\\frac{y_i}{\\hat{y}_i}\\right)-\\left(y_i-\\hat{y}_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中平方误差与绝对误差是大家非常熟悉的概念，作为分枝标准，平方误差比绝对误差更敏感（类似于信息嫡比基尼系数更敏感），并且在计算上平方误差比绝对误差快很多。泊松偏差则是适用于一个特殊场景的：当需要预测的标签全部为正整数时，标签的分布可以被认为是类似于泊松分布的。正整数预测在实际应用中非常常见，比如预测点击量、预测客户/离职人数、预测销售量等。我们现在正在使用的数据（房价预测），也可能比较适合于泊松偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，当我们选择不同的criterion之后，决策树的feature_importances_也会随之变化，因为在sklearn当中，\n",
    "feature_importances_是特征对criterion下降量的总贡献量，因此不同的criterion可能得到不同的特征重要性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对我们来说，选择criterion的唯一指标就是最终的交又验证结果一一无论理论是如何说明的，我们只取令随机森林的预测结果最好的criterion。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 调节树结构来控制过拟合\n",
    "> maxdepth\n",
    "\n",
    "最粗扩的剪枝方式，从树结构层面来看，对随机森林抗过拟合能力影响最大的参数。max_depth的默认值为None，也就是不限深度。因此当随机森林表现为过拟合时，选择一个小的max_depth会很有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> max_leaf_nodes与min_sample_split\n",
    "\n",
    "比max_depth更精细的减枝方式，但限制叶子数量和分枝，既可以实现微调，也可以实现大刀阔斧的剪枝。max_leaf_nodes的默认值为None，即不限叶子数量。min_sample_split的默认值为2，等同于不限制分枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> min_impurity_decrease\n",
    "\n",
    "最精细的减枝方式，可以根据不纯度下降的程度减掉相应的叶子。默认值为0，因此是个相当有空间的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 弱分类器数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators是森林中树木的数量，即弱评估器的数量，在sklearn中默认100，它是唯一一个对随机森林而言填的参数。n_estimators对随机森林模型的精确程度、复杂度、学习能力、过拟合情况、需要的计算量和计算时间都有很大的影响，因此n_estimators往往是我们在调整随机森林时第一个需要确认的参数。对单一决策树而言，模型复杂度由树结构(树深、树宽、树上的叶子数量等)与数据量(样本量、特征量)决定，而对随机森林而言，模型复杂度由森林中树的数量、树结构与数据量决定，其中树的数量越多，模型越复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还记得讲解决策树与逻辑回归时我们绘制的这张图像么？当模型复杂度上升时，模型的泛化能力会先增加再下降（相对的泛化误差会先下降再上升），我们需要找到模型泛化能力最佳的复杂度。在实际进行训练时，最佳复杂度往往是一个比较明显的转折点，当复杂度高于最佳复杂度时，模型的泛化误差要么开始上升，要么不再下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-145.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对随机森林而言，该图像的横坐标可以被无缝切换为参数n_estimators上的值。当n_estimators越大时："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 模型的复杂程度上升，泛化能先增强再减弱（或不变）\n",
    "* 模型的学习能力越来越强，在训练集上的分数可能越来越高，过拟合风险越来越高\n",
    "* 模型需要的算力和内存越来越多\n",
    "* 模型训练的时间会越来越长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此在调整n_estimators时，我们总是渴望在模型效果与训练难度之间取得平衡，同时我们还需要使用交叉验证来随时关注模型过拟合的情况。在sklearn现在的版本中，n_estimators的默认值为100，个人电脑能够容忍的n_estimators数量大约在200~1000左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(cvresult, key):\n",
    "    return (abs(cvresult[key])**0.5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 弱分类器训练的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还记得决策树是如何分枝的吗？对每个特征决策树都会找到不纯度下降程度最大的节点进行分枝，因此原则上来说，只要给出数据一致、并且不对决策树进行减枝的话，决策树的结构一定是完全相同的。对集成算法来说，平均多棵相同的决策树的结果并没有意义，因此集成算法中每棵树必然是不同的树，Bagging算法是依赖于随机抽样数据来实现这一点的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林会从提供的数据中随机抽样出不同的子集，用于建立多棵不同的决策树，最终再按照Bagging的规则对众多决策树的结果进行集成。因此在随机森林回归器的参数当中，有数个关于数据随机抽样的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 样本的随机抽样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> boostrap,oob_score,max_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstrap参数的输入为布尔值，默认True，控制是否在每次建立决策树之前对数据进行随机抽样。如果设置为False，则表示每次都使用全部样本进行建树，如果为True，则随机抽样建树。从语言的意义上来看，boostrap可以指代任意类型的随机抽样，但在髓机森林中它特指有**放回随机抽样技术**，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下图所示，在一个含有m个样本的原始训练集中，我们进行随机采样。每次采样一个样本，并在抽取下一个样本之前将该样本放回原始训练集，也就是说下次采样时这个样本依然可能被采集到，这样采集max_samples次，最格得到max_amples个样本组成的自助集，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-146.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常来说，max_samples是等于m的（行业惯例），也就是抽样数据集的大小与原始数据集一致，但是如果原始数据集太大、或者太小，我们也可以自由调整max_samples的大小。由于是随机采样，这样每次的自助集和原始数据集不同，和其他的采样集也是不同的。这样我们就可以自由创造取之不尽用之不竭，并且互不相同的自助集，用这些自助集来训练我们的弱分类器，我们的弱分类器自然也就各不相同了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而有放回抽样也会有自己的问题。由于是有放回，一些样本可能在同一个自助集中出现多次，而其他一些却\n",
    "可能被忽略。当抽样次数足够多、且原始数据集足够大时，自助集大约平均会包含全数据的63%，这个数字是\n",
    "有数学依据的。因为在max_samples次抽样中，一个样本被抽到某个自助集中的概率为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "1-\\left(1-\\frac{1}{m}\\right)^{\\text{max\\_samples}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个式子是怎么来的呢？对于任意一个样本而言："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一次抽样时抽到该样本的概率为 $\\frac{1}{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一次抽样时抽不到该样本的概率为 $1-\\frac{1}{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总共抽样max_samples次，一次也没有抽到该样本的概率就是 $1-\\left(\\frac{1}{m}\\right)^{\\text{max\\_samples}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此1减去该概率，就是一个样本在抽样中一定会被抽到某个自助集的概率。当m刚好等于max_samples时，公式可以被修改为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "1-\\left(1-\\frac{1}{m}\\right)^{\\text{m\\_samples}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这明显是一个经典的极限问题，由洛必达法则(L\"Hopital'srule)我们可知：当m足够大时（接近极限时）这个概率收敛于1-(1/e)，其中e是自然常数，整体概率约等于0.632。因此，会有约37%的训练数据被浪费掉，没有参与建模，这些数据被称为袋外数据(outofbagdata，简写为oob)。在实际使用随机森林时，袋外数据常常被我们当做验证集使用，所以我们或许可以不做交叉验证、不分割数据集，而只依赖于袋外数据来测试我们的模型即可。当然，这也不是绝对的，当树的数n_estimators不足，或者max_samples太小时，很可能就没有数据掉落在袋外，自然也有无法使用oob数据来作为验证集了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在随机森林回归器中，当boostrap=True时，我们可以使用参数oob_score和max_samples，其中："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oob_score控制是否使用袋外数据进行验证，输入为布尔值，默认为False，如果希望使用袋外数据进行验证，修改为True即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_samples表示自助集的大小，可以输入整数、浮点数或None，默认为None。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用袋外数据时，我们可以用随机森林的另一个重要属性：oob_score_来查看我们的在袋外数据上测试的结果，遗憾的是我们无法调整oob_score_输出的评估指标，它默认是R2.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 特征的随机抽样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据抽样还有另一个维度：对特征的抽样。在学习决策树时，我们已经学习过对特征进行抽样的参数max_features，在随机森林中max_features的用法与决策树中完全一致，其输入也与决策树完全一致："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 输入整数，表示每次分枝时随机抽取max_features个特征\n",
    "\n",
    "> 输入浮点数，表示每次分枝时抽取round(max_features*n_features)个特征\n",
    "\n",
    "> 输入\"auto\"或者None，表示每次分枝时使用全部特征n_features\n",
    "\n",
    "> 输入\"sqrt\"，表示每次分枝时使用sqrt(n_features)\n",
    "\n",
    "> 输入\"log2\"，表示每次分枝时使用log2(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-147.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难发现，sqrt(n_features)和log2(n_features)都会返回一个比原始特征量小很多的数，但一般情况下log2返回的值比sqrt返回的值更小，因此如果我们想要树之间的差异更大，我们可以设置模式为log2。在实际使用时，我们往往会先使用上述的文字输入，观察模型的结果，然后再在有效的范围附近进行网格搜素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**需要注意的是，无论对数据进行怎样的抽样，我们能够控制的都只是建立单棵树时的数据而已**。在总数据量有限的情况下，单棵树使用的数据量越大，每一棵树使用的数据就会越相似，每棵树的结构也就会越相似，bagging的效果难以发挥、模型也很容易变得过拟合。因此，当数据量足够时，我们往往会消减单棵树使用的数据量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 随机抽样的模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在决策树当中，我们已经学习过控制随机模式的参数random_state，这个参数是\"随机数种子\"，它控制决策树当中多个具有随机性的流程。在sklearn实现的随机森林当中，决策树上也存在众多有随机性的流程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 「强制」随机抽取每棵树建立时分枝用的特征，抽取的数量可由参数max_features决定\n",
    "* 「强制」随机排序每棵树分枝时所用的特征\n",
    "* 「可选」随机抽取每棵树建立时训练用的样本，抽取的比例可由参数max_samples决定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此每次使用随机森林类时，我们建立的集成算法都是不同的，在同一个数据集上多次建树自然也会产生不同的模型结果。因此在工程部署和教学当中，我们在建树的第一步总是会先设置随机数种子为一个固定值，让算法固定下来。在设置的时候，需要注意两个问题："
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
