{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.3 线性建模回归实验\n",
    "\n",
    "## 12.3.1 深度学习建模流程\n",
    "\n",
    "数据准备就绪，接下来就是建模实验环节，在实际深度学习建模过程中，无论是手动实现还是调库实现，我们都需要遵循深度学习建模一般流程。在此前的学习过程中，我们曾两次体积深度学习建模流程，结合此前学习内容，我们先进行简单回顾。\n",
    "\n",
    "![Alt text](image-25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据此流程，我们进行线性回归建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3.2 线性回归的手动实现\n",
    "\n",
    "首先，考虑如何手动实现建模过程\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机模块\n",
    "import random\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchLearning import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 一个cell输出多个结果\n",
    "writer = SummaryWriter(log_dir= 'reg_loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 生成数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用此前的数据集生成函数，创建一个真实关系为 $ y = 2x_{1} - x_{2} + 1 $ ，且扰动项不是很大的回归类数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(420)\n",
    "features, labels = tensorGenReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4098588772.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    tensorGenReg ?\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tensorGenReg ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 建模流程\n",
    "\n",
    "* Stage 1.模型选择\n",
    "\n",
    "围绕建模目标，我们可以构建一个只包含一层的神经网络进行建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-26.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(x, w):\n",
    "    return torch.mm(x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stage 2. 确定目标函数\n",
    "\n",
    "和此前一样，我们使用MSE作为损失函数，也就是目标函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    num_ = y.numel()\n",
    "    sse = torch.sum((y_hat.reshape(-1, 1) - y.reshape(-1, 1)) ** 2)\n",
    "    return sse/num_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stage 3. 定义优化算法\n",
    "\n",
    "此处我们采用小批量梯度下降进行求解，每一次迭代过程都是（参数 - 学习率*梯度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr):\n",
    "    params.data -= lr * params.grad\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    关于可微张量的 in-place operation（对原对象修改操作）的相关讨论\n",
    "\n",
    "（1） 正常情况下，可微张量的 in-place operation会导致系统无法区分叶节点和其他节点的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(2., requires_grad= True)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开启可微后，w的所有计算都会被纳入到计算图中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = w* 2\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但如果在计算过程中，我们使用 in-place operation，让新生成的值替换w原始值，则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m2.0\u001b[39m, requires_grad\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m w \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m w\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad= True)\n",
    "w -= w*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从报错信息可知，PyTorch中不允许叶节点使用 in-place operation，根本原因是会造成叶节点和其他节点类型混乱。不过，虽然可微张量不允许 in-place operation，但却可以通过其他方法对w进行修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2） 叶节点数值修改方法\n",
    "\n",
    "当然，如果出现了一定要修改叶节点的取值的情况，典型的如梯度下降过程利用梯度值修改参数值时，可以使用此前介绍的暂停追踪的方法，如使用 with torch.no_grad（）语句或者torch.detach()方法，使得修改叶节点数值时暂停追踪，然后再生成新的叶节点带入计算，如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad= True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2., requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用 with torch.no_grad（）暂停追踪\n",
    "with torch.no_grad():\n",
    "    w -= w * 2\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad= True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用detach生成新变量\n",
    "w.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w -= w * 2\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2., requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.requires_grad = True\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，此处我们介绍另一种方法，.data来返回可微张量的取值，从而避免在修改过程中被追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad= True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.data  # 查看张量的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2., requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.data -= w * 2     # 对其数值进行修改\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.00012803004938177764\n",
      "epoch: 2, loss: 0.00010162403486901894\n",
      "epoch: 3, loss: 0.00010135798947885633\n"
     ]
    }
   ],
   "source": [
    "# 设置随机数种子\n",
    "torch.manual_seed(420)\n",
    "\n",
    "# 初始化核心参数\n",
    "batch_size = 10                             # 每一个小批的数量\n",
    "lr = 0.03                                   # 学习率\n",
    "num_epochs = 3                              # 训练过程遍历几次数据\n",
    "w = torch.zeros(3, 1, requires_grad= True)  # 随机设置初始权重\n",
    "\n",
    "# 参与训练的模型方程\n",
    "net = linreg                                # 使用回归方程\n",
    "loss = squared_loss                         # MSE作为损失函数\n",
    "\n",
    "# 模型训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w), y)\n",
    "        l.backward()\n",
    "        sgd(w, lr)\n",
    "    train_1 = loss(net(features, w), labels)\n",
    "    print(f'epoch: {epoch + 1}, loss: {train_1}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0001],\n",
       "        [-1.0006],\n",
       "        [-1.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们也可以使用tensorboard记录上述迭代过程中loss的变化过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir= 'reg_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化核心参数\n",
    "batch_size = 10    \n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "w = torch.zeros(3, 1, requires_grad= True)\n",
    "\n",
    "# 参与训练的模型方程\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "# 模型训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w), y)\n",
    "        l.backward()\n",
    "        sgd(w, lr)\n",
    "    train_1 = loss(net(features, w), labels)\n",
    "    writer.add_scalar('mul', train_1, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3.3 线性回归的快速实现\n",
    "\n",
    "当然，我们可以按照此前课程介绍的，通过调用PyTorch中的函数和类，直接完成建模。当然，该过程也是严格按照此前介绍的深度学习建模流程完成的模型构建，相关变量的符号也沿用了Lesson 11中的书写标准。\n",
    "\n",
    "不过，值得一提的是，由于深度学习的特殊性，深度学习的建模流程对于初学者来说并不简单，很多时候我们既无法对实际的数据进行表格式的查看，也无法精确的控制模型内部的每一步运行，外加需要创建大量的类（无论是读取数据还是建模），以及大规模的参数输入，都对初学者的学习造成了不小的麻烦。因此，通过接下来的调库建模练习，也希望学员能够进一步熟练掌握PyTorch框架的常用建模函数和类，从而为后续的学习打下良好的代码基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义核心参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10         # 每一个小批的数量\n",
    "lr = 0.03               # 学习率\n",
    "num_epochs = 3          # 训练过程遍历几次数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机数种子\n",
    "torch.manual_seed(420)\n",
    "\n",
    "# 创建数据集\n",
    "features, labels = tensorGenReg()\n",
    "features = features[:, :-1]         # 剔除最后全是1的列\n",
    "data = TensorDataset(features, labels)\n",
    "batchdata = DataLoader(data, batch_size= batch_size, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0070,  0.5044],\n",
       "        [ 0.6704, -0.3829],\n",
       "        [ 0.0302,  0.3826],\n",
       "        ...,\n",
       "        [-0.9164, -0.6087],\n",
       "        [ 0.7815,  1.2865],\n",
       "        [ 1.4819,  1.1390]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stage 1.定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self, in_features = 2, out_features = 1):  # 定义模型的点线结构\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_features, out_features, bias= True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear_1(x)\n",
    "        return out\n",
    "\n",
    "# 实例化模型\n",
    "LR_model = LR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stage 2. 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stage 3.定义优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(LR_model.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stage 4.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, criterion, optimizer, batchadata, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in batchadata:\n",
    "            yhat = net.forward(X)\n",
    "            loss = criterion(yhat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        writer.add_scalar('loss', loss, global_step= epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(420)\n",
    "\n",
    "fit(net= LR_model,\n",
    "    criterion= criterion,\n",
    "    optimizer= optimizer,\n",
    "    batchadata= batchdata,\n",
    "    epochs= num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看模型训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LR(\n",
       "  (linear_1): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 2.0006, -1.0000]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.9992], requires_grad=True)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型参数\n",
    "list(LR_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算MSE\n",
    "criterion(LR_model(features), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于数据本身就是按照 $ y = 2x_{1} - x_{2} + 1 $ 基本规律加上扰动项构建的，因此通过训练完成的参数可以看出模型效果较好。当然，真实场景下我们无法从上帝视角获得真实的数据分布规律，然后通过比对模型来判断模型好坏，此时我们就需要明确模型评估指标。\n",
    "\n",
    "当然，我们也可以通过add_graph方法，在writer中添加上述模型的记录图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(LR_model, (features,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 简单线性回归的局限性\n",
    "\n",
    "此处我们进一步进行简单实验，当自变量和因变量满足最高次方为2次方的多项式函数关系时，或者扰动项增加时，简单线性回归误差将迅速增大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.1917, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置随机数种子\n",
    "torch.manual_seed(420)\n",
    "\n",
    "# 创建数据集\n",
    "features, labels = tensorGenReg(deg= 2)\n",
    "features = features[:, :-1]\n",
    "data = TensorDataset(features, labels)\n",
    "batchdata = DataLoader(data, batch_size= batch_size, shuffle= True)\n",
    "\n",
    "# 模型实例化\n",
    "LR_model = LR()\n",
    "\n",
    "# 定义优化算法\n",
    "optimizer = optim.SGD(LR_model.parameters(), lr= 0.03)\n",
    "\n",
    "# 模型训练\n",
    "fit(net= LR_model,\n",
    "    criterion= criterion,\n",
    "    optimizer= optimizer,\n",
    "    batchadata= batchdata,\n",
    "    epochs= num_epochs)\n",
    "\n",
    "# MSE结果查看\n",
    "criterion(LR_model(features), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0471, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置随机数种子\n",
    "torch.manual_seed(420)\n",
    "\n",
    "# 创建数据集\n",
    "features, labels = tensorGenReg(delta= 2)\n",
    "features = features[:, :-1]\n",
    "data = TensorDataset(features, labels)\n",
    "batchdata = DataLoader(data, batch_size= batch_size, shuffle= True)\n",
    "\n",
    "# 模型实例化\n",
    "LR_model = LR()\n",
    "\n",
    "# 定义优化算法\n",
    "optimizer = optim.SGD(LR_model.parameters(), lr= 0.03)\n",
    "\n",
    "# 模型训练\n",
    "fit(net= LR_model,\n",
    "    criterion= criterion,\n",
    "    optimizer= optimizer,\n",
    "    batchadata= batchdata,\n",
    "    epochs= num_epochs)\n",
    "\n",
    "# MSE结果查看\n",
    "criterion(LR_model(features), labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
