{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 在MINIST-FASHION上实现神经网络的学习流程\n",
    "\n",
    "本节课我们讲解了神经网络使用小批量随机梯度下降进行迭代的流程，现在我们要整合本节课中所有的代码实现一个完整的训练流程，首先要梳理一下整个流程：\n",
    "\n",
    "1. 设置步长 $lr$,动量值 $gamma$，迭代次数 $epochs$，$batchsize$等信息，（如果需要）设置初始权重 $ w_{0}$\n",
    "2. 导入数据集，将数据切分成batches\n",
    "3. 定义神经网络架构\n",
    "4. 定义损失函数L（w），如果需要的话，将损失函数调成凸函数，以便求解最小值\n",
    "5. 定义所使用的优化算法\n",
    "6. 开始在epoches和batch上循环，执行优化算法：\n",
    "    a. 调整数据结构，确定数据能够在神经网络，损失函数和优化算法中顺利运行\n",
    "    b.完成向前传播，在损失函数L（w）上对每一个w求偏导数\n",
    "    c.迭代当前权重\n",
    "    d.清空本轮梯度\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\t    # 数据处理模块\n",
    "\n",
    "# dataloader, tensordataset - 对数据结构、归纳方式进行变换\n",
    "# torchvision.transforms - 对数据集的数字本身进行修改\n",
    "\n",
    "mnist = torchvision.datasets.FashionMNIST(root=\"./data\"   # 你的计算机上的某个目录\n",
    "                                        , download= True\n",
    "                                        , train= True\n",
    "                                        , transform= transforms.ToTensor()\n",
    "                                        )         # 实例化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库、包\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss as CEL \n",
    "\n",
    "# 设置超参数\n",
    "lr = 0.01\n",
    "gamma = 0.5\n",
    "epochs = 5\n",
    "batch_size = 1000\n",
    "\n",
    "# 导入数据，并将数据切分成batches\n",
    "mnist = torchvision.datasets.FashionMNIST(root= './data'\n",
    "                                          , download= True\n",
    "                                          , train= True\n",
    "                                          , transform= transforms.ToTensor()\n",
    "                                          )\n",
    "mnist_dataset = DataLoader(mnist\n",
    "                           ,batch_size= batch_size\n",
    "                           , shuffle= True\n",
    "                            , drop_last= True\n",
    "                            )\n",
    "\n",
    "in_put = mnist.data.shape[0]                # in_put输入有问题，输入层应该是28*28\n",
    "out_put = mnist.targets.unique()            # out_put输出也有问题，应该用len()函数\n",
    "\n",
    "# 神经网络架构\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features=10, out_features=2):\n",
    "        super().__init__()\n",
    "        self.Linear_1 = nn.Linear(in_features, 13)\n",
    "        self.Linear_2 = nn.Linear(13, 8)\n",
    "        self.out = nn.Linear(8, out_features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z1 = self.Linear_1(X)           # 要考虑考虑X是否需要进行形状的变换，因为只能传入一维的数据\n",
    "        sigma_1 = F.sigmoid(z1)\n",
    "        z2 = self.Linear_2(sigma_1)\n",
    "        sigma_2 = F.relu(z2)\n",
    "        zhat = self.out(sigma_2)\n",
    "        return zhat\n",
    "\n",
    "# 损失函数，反向传播，优化算法可以考虑放置在同一个函数里面\n",
    "\n",
    "model = Model(in_put, out_put)\n",
    "# 定义损失函数L(w)\n",
    "def Loss(zhat, y):\n",
    "    criterion = CEL()       # 如果要输入模型的准确率，就不能用CEL损失函数进行计算\n",
    "    loss = criterion(zhat, y)\n",
    "    return loss\n",
    "optimizer = optim.SGD(model.parameters(), lr= lr)\n",
    "for epochs in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(mnist_dataset):\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 向前传播\n",
    "        zhat = model.forward(data)\n",
    "        # 计算损失函数\n",
    "        loss = Loss(zhat, target)\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        optimizer.step\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确率仅仅为47%，这是由于神经网络的结构，以及激活函数导致的：\n",
    "\n",
    "Sigmoid激活函数在图像识别分类任务中不太常用，而更常见的选择是Rectified Linear Unit (ReLU) 激活函数及其变种，如Leaky ReLU、Parametric ReLU等。这是因为Sigmoid函数在某些情况下存在梯度消失的问题，导致深层神经网络的训练变得困难，尤其是在反向传播过程中，梯度逐渐减小，可能导致网络权重更新变得缓慢，从而降低了模型的训练速度和性能。\n",
    "\n",
    "相比之下，ReLU函数具有以下优点：\n",
    "\n",
    "* 避免梯度消失问题： ReLU函数在正数区间的梯度恒为1，这有助于防止梯度消失问题，使得深层网络的训练更加稳定和高效。\n",
    "\n",
    "* 计算高效： ReLU函数的计算相对简单，只需要比较输入是否大于零即可，相比于Sigmoid函数和双曲正切函数的计算更快。\n",
    "\n",
    "* 稀疏激活性： ReLU的负值部分会变为零，从而使得神经元具有一定的稀疏激活性，这有助于模型的泛化能力。\n",
    "\n",
    "虽然Sigmoid函数在输出范围上有界（0到1之间），适用于二分类问题，但在深度神经网络中，ReLU激活函数通常更适合处理图像识别分类任务。如果你关心输出范围，你可以使用输出范围在[0, 1]的输出层来处理分类问题，例如使用Sigmoid激活函数，但是在隐藏层中，使用ReLU激活函数会更为常见和推荐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改后的代码\n",
    "\n",
    "# 导入库、包\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss as CEL \n",
    "\n",
    "# 设置超参数\n",
    "lr = 0.01\n",
    "gamma = 0.5\n",
    "epochs = 5\n",
    "batch_size = 124\n",
    "\n",
    "# 导入数据，并将数据切分成batches\n",
    "mnist = torchvision.datasets.FashionMNIST(root= './data'\n",
    "                                          , download= True\n",
    "                                          , train= True\n",
    "                                          , transform= transforms.ToTensor()\n",
    "                                          )\n",
    "batch_data = DataLoader(mnist\n",
    "                           ,batch_size= batch_size\n",
    "                           , shuffle= True\n",
    "                            , drop_last= True\n",
    "                            )\n",
    "\n",
    "in_put = mnist.data[0].numel()\n",
    "out_put = len(mnist.targets.unique())\n",
    "\n",
    "# 神经网络架构\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features=10, out_features=2):\n",
    "        super().__init__()\n",
    "        self.Linear_1 = nn.Linear(in_features, 13)\n",
    "        self.Linear_2 = nn.Linear(13, 8)\n",
    "        self.out = nn.Linear(8, out_features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z1 = self.Linear_1(X.view(-1, 28*28))\n",
    "        sigma_1 = F.sigmoid(z1)\n",
    "        z2 = self.Linear_2(sigma_1)\n",
    "        sigma_2 = F.relu(z2)\n",
    "        zhat = self.out(sigma_2)\n",
    "        sigma = F.log_softmax(zhat, dim = 1)\n",
    "        return sigma\n",
    "\n",
    "def fit_(net, batch_data,lr= 0.05, gamma= 0.5,epochs= 5):\n",
    "  criterion = nn.NLLLoss()\n",
    "  opt = optim.SGD(net.parameters()\n",
    "            ,lr = lr\n",
    "            , momentum = gamma)\n",
    "  samples = 0 # 循环开始之前，模型一个样本都没见过\n",
    "  correct = 0 # 循环开始之前，训练样本为0\n",
    "  for epoch in range(epochs):\n",
    "    for batch_idx, (x,y) in enumerate(batch_data):\n",
    "      y = y.view(x.shape[0])\n",
    "      sigma = net.forward(x)\n",
    "      loss = criterion(sigma, y)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      opt.zero_grad()\n",
    "\n",
    "      # 求解准确率，全部判断正确的样本数据/已经看过的总样本量\n",
    "      yhat = torch.max(sigma, 1)[1]\n",
    "      correct += torch.sum(yhat == y).item()\n",
    "      samples += x.shape[0]\n",
    "\n",
    "      if (batch_idx +1) % 125 == 0:\n",
    "        print(\"Epoch{}:[{}/{} {:.0f}%, Loss:{:.6f},Accuracy:{:.3f}]\".format(epoch+1\n",
    "                      , samples\n",
    "                      , epochs*len(batch_data.dataset)\n",
    "                      , 100*samples/(epochs*len(batch_data.dataset))\n",
    "                      , loss.data.item()\n",
    "                      , float(100*correct/samples)))\n",
    "\n",
    "# 训练与评估\n",
    "torch.manual_seed(1412)\n",
    "net = Model(in_features = in_put, out_features= out_put)\n",
    "fit_(net, batch_data, lr= lr, gamma= gamma, epochs = epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "菜菜的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 导入库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 确定数据、超参数\n",
    "lr = 0.15\n",
    "gamma = 0.8\n",
    "epochs = 5\n",
    "bs = 128\n",
    "\n",
    "mnist = torchvision.datasets.FashionMNIST(root= './data'\n",
    "                                          , download= True\n",
    "                                          , train= True\n",
    "                                          , transform= transforms.ToTensor()\n",
    "                                          )\n",
    "batch_data = DataLoader(mnist\n",
    "                        , batch_size= bs\n",
    "                        , shuffle= True\n",
    "                        , drop_last= True)\n",
    "\n",
    "in_put = mnist.data[0].numel()\n",
    "out_put = len(mnist.targets.unique())\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, in_features = 10, out_features = 2):\n",
    "      super().__init__()\n",
    "      self.linear1 = nn.Linear(in_features, 128, bias = False)\n",
    "      self.out = nn.Linear(128, out_features, bias = False)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    X = x.view(-1, 28*28)   # view(-1),表示需要对数据结构进行一个改变，-1作为占位符，表示请pytorch帮助我们自动计算-1处的维度\n",
    "    sigma1 = torch.relu(self.linear1(X))\n",
    "    sigma2 = F.log_softmax(self.out(sigma1), dim = 1)\n",
    "    return sigma2\n",
    "\n",
    "def fit_(net, batch_data, lr=0.01, epochs=5, gamma= 1.0):\n",
    "  criterion = nn.NLLLoss()\n",
    "  opt = optim.SGD(net.parameters()\n",
    "            , lr = lr\n",
    "            , momentum= gamma)\n",
    "  samples = 0 # 循环开始之前，模型一个样本没见过\n",
    "  correct = 0 # 循环开始之前，训练样本为0\n",
    "  for epoch in range(epochs):\n",
    "    for batch_idx, (x,y) in enumerate(batch_data):\n",
    "      # y = y.view(x.shape[0]) 降维\n",
    "      sigma = net.forward(x)\n",
    "      loss = criterion(sigma, y)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      opt.zero_grad()\n",
    "\n",
    "      # 求解准确率，全部判断正确的样本数据/已经看过的总样本量\n",
    "      yhat = torch.max(sigma, 1)[1]   # torch.max函数结果中的索引为1的部分\n",
    "      correct += torch.sum(yhat == y).item()\n",
    "\n",
    "      samples += x.shape[0]\n",
    "      if (batch_idx + 1) % 125 == 0:\n",
    "        print(\"Epoch{}:[{}/{} {:.0f}%, Loss:{:.6f},Accuracy:{:.3f}]\".format(epoch+1\n",
    "                      , samples\n",
    "                      , epochs*len(batch_data.dataset)\n",
    "                      , 100*samples/(epochs*len(batch_data.dataset))\n",
    "                      , loss.data.item()\n",
    "                      , float(100*correct/samples)))  # 分子代表：已经查看过的数据有多少，分母代表在现有的epochs设置，模型一共需要查看多少数据\n",
    "        \n",
    "# 训练与评估\n",
    "torch.manual_seed(1412)\n",
    "net = Model(in_features= in_put, out_features= out_put)\n",
    "fit_(net, batch_data, lr =lr, epochs= epochs, gamma= gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注释：\n",
    "1. torch.max(): 结果是一个包含两个张量的元组，第一个包含了最大值，第二个包含了最大值的索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在已经完成了一个最基本的、神经网络训练并查看训练结果的代码，是不是感觉已经获得了很多知识呢？我们的模型最后得到的结果属于中规中矩，毕竟我们设置的网络结构只是最普通的全连接层，并且我们没有对数据进行任何处理或增强。已经成熟的神经网络架构可以很轻易在MINST-FASHION数据集上获得99%的准确率，因此我们还有很长的路要走。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
