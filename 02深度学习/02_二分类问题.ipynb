{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 二分类神经网络的理论基础\n",
    "\n",
    "线性回归是统计学经典算法，它能够拟合出来一条直线来描述变量之间的线性关系。但在实际中，变量之间的关系通常都不是一条直线，二十呈现出某种曲线关系。在统计学的历史中，为了让统计学模型能更好地拟合曲线，统计学家们在线性回归的方程两边引入了联系函数，对线性回归的方程做出了各种各样的变换，并将这些变换后的方程称为“广义线性回归”。其中比较著名的有等式两边同时取对数的对数函数回归，同时取指数的S形函数回归等。\n",
    "\n",
    "$\\begin{aligned} & y=a x+b \\rightarrow \\ln y=\\ln (a x+b) \\\\ & y=a x+b \\rightarrow e^y=e^{a x+b}\\end{aligned}$\n",
    "\n",
    "在探索的过程中，一种奇特的变化吸引了统计学家们的注意，这个变化就是sigmoid函数带来的变化。sigmoid函数的公式如下：\n",
    "\n",
    "$\\sigma=\\operatorname{Sigmoid}(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "其图像如下：\n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "从图像中可以看出，自变量z趋于正无穷，因变量趋于1；趋于负无穷，因变量趋于0\n",
    "\n",
    "这样的性质，让sigmoid函数拥有将连续性变量z转化为离散型变量o的力量，这也就是化回归算法为分类算法的力量。\n",
    "\n",
    "具体怎么操作呢？只要将线性回归方程的结果作为自变量带入sigmoid函数，得出的数据就一定是（0， 1）之间的值。此时，只要我们设定一个阈值（比如0.5），规定o大于0.5时，预测结果为1类，o小于0.5时，预测结果为0类，则可以顺利将回归算法转化为分类算法。\n",
    "\n",
    "更神奇的是，当我们对线性回归的结果取sigmoid函数之后，只要再进行以下操作：\n",
    "\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "![Alt text](image-3.png)\n",
    "\n",
    "不难发现，让o取对数几率后所得到的值就是我们线性回归的z！因为这个性质，在等号两边加sigmoid的算法被称为“对数几率回归”,在英文中就是Logistic Regression，就是逻辑回归。逻辑回归可能是广义线性回归中最广为人知的算法，它是一个叫做回归，实际上却总是被用来做分类的算法，对机器学习和深度学习都有重大意义。\n",
    "\n",
    "加餐：\n",
    "\n",
    "![Alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例：使用tensor来实现\n",
    "\n",
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4502, 0.4875, 0.4875, 0.5250])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype= torch.float32)\n",
    "andgate = torch.tensor([[0], [0], [0], [1]], dtype= torch.float32)\n",
    "w = torch.tensor([-0.2, 0.15, 0.15], dtype= torch.float32)\n",
    "\n",
    "def LogisticR(X, w):\n",
    "    zhat = torch.mv(X, w)\n",
    "    sigma = 1/(1 +torch.exp(-zhat))\n",
    "    andhat = torch.tensor([int(x) for x in sigma >= 0.5], dtype= torch.float32)\n",
    "    return sigma, andhat\n",
    "\n",
    "sigma, andhat = LogisticR(X, w)\n",
    "sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "andhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 符号函数sign, Relu, Tanh\n",
    "\n",
    "略......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.使用PyTorch来搭建二分类网络\n",
    "\n",
    "在PyTorch中，我们使用类ntorch.nn.Linear类来实现单层回归神经网络，不过需要注意的是，它可不是代表单层回归神经网络这个算法。\n",
    "\n",
    "torch.nn是包含了构筑神经网络结构基本元素的包，在这个包中，你可以找到任意的神经网络层。这些神经网络层都是nn.Model这个大类中的子类。我们的torch.nn.Linear就是神经网络中的“线性层”，它可以实现形如Z = XW的加和功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 0], [1, 0], [0, 1], [1, 1]], dtype= torch.float32)\n",
    "torch.random.manual_seed(420)\n",
    "dense = torch.nn.Linear(2, 1)\n",
    "zhat =dense(X)\n",
    "sigma = F.sigmoid(zhat)\n",
    "y = torch.tensor([int(x) for x in sigma >= 0.5])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], grad_fn=<SignBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sign(zhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6730],\n",
       "        [1.1048],\n",
       "        [0.2473],\n",
       "        [0.6792]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.relu(zhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5869],\n",
       "        [0.8022],\n",
       "        [0.2424],\n",
       "        [0.5910]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.tanh(zhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
