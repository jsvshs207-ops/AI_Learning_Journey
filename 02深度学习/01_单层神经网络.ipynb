{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 手动构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# z = b + w1*x1 +w2*x2, x*w\n",
    "x = torch.tensor([[1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype= torch.float32)\n",
    "# 参数初始化\n",
    "w = torch.tensor([-0.2, 0.15, 0.15], dtype= torch.float32)    # (b, w1, w2)\n",
    "z = torch.tensor([[-0.2], [-0.05], [-0.05], [0.1]], dtype = torch.float32)\n",
    "def LinearR(X, w):\n",
    "    zhat = torch.mv(X, w)\n",
    "    return zhat\n",
    "\n",
    "zhat = LinearR(X= x, w= w)\n",
    "z = z.view(4)\n",
    "zhat\n",
    "torch.allclose(z, zhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor - 判断你输入的数据是什么类型，然后根据你输入的数据来确定张量数据类型\n",
    "# torch.Tensor - 无论输入什么数据，都无脑使用float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义张量一定要定义类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float32可能会有细微误差，要提高精度可以用float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allclose（）无限接近，忽略细微误差进行比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 利用PyTorch构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们使用nn.Linear来实现单层回归神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0, 0], [1, 0], [0, 1], [1, 1]], dtype= torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5637],\n",
       "        [-1.1240],\n",
       "        [ 0.0359],\n",
       "        [-0.5245]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w\n",
    "# 实例化nn.Linear\n",
    "output = torch.nn.Linear(2, 1)      # 可以添加参数 bias = Flase,这样就可以控制不生成 b\n",
    "# 2:上一层神经元个数（上一层神经元输入给本层神经元的个数）\n",
    "# 1：本层神经元个数（接受传输数据的神经元个数）\n",
    "# 自动生成了系数w 和偏置 b\n",
    "\n",
    "zhat = output(x)\n",
    "zhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么样，代码是不是异常简单呢？但在这段代码中，却有许多细节需要声明：\n",
    "\n",
    "*  nn.Linear是一个类，在这里代表了输出层，所以我使用output作为变量名，output = 的这一行相当于是类的实例化过程\n",
    "\n",
    "*  实例化的时候，nn.Linear需要传入两个参数，分别是（上一层的神经元个数，这一层的神经元个数）。上一层是输入层，因此神经元个数由特征的个数决定（2个）。这一层是输出层，作为回归神经网络，输出层只有一个神经元。因此nn.Linear中输入的是（2， 1）\n",
    "\n",
    "*  我们只定义了x, 没有定义w和b。所有nn.Module的子类，形如nn.XXX的层，都会在实例化的同时随机生成w和b的初始值。所以实例化后，我们就可以调用以下属性来查看生成的w和b："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5603,  0.5996]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.5637], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  由于w和b是随机生成的，所以同样的代码多次运行后的结果是不一致的。如果我们希望控制随机性，则可以使用torch中的random类。如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4318, -0.4256]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机数种子\n",
    "torch.random.manual_seed(420)       # 人为设置随机数种子,这样bias和w就固定了\n",
    "output = torch.nn.Linear(2, 1)\n",
    "output.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.6730], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 由于不需要定义常量b，因此在特征张量中，也不需要留出与常数项相乘的X0的那一列。在输入数据时，我们只输入了两个特诊X1与X2\n",
    "\n",
    "* 输入层只有一层，并且输入层的结构（神经元个数）由输入的特征张量X决定，因此在PyTorch中构筑神经网络时，不需要定义输入层\n",
    "\n",
    "* 实例化后，将特征张量输入到实例化后的类中，即可得到输出层的输出结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
