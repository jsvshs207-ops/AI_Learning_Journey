{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 移动坐标点\n",
    "\n",
    "## 1.1 走出第一步\n",
    "\n",
    "有了大小和方向，接下来我们就可以走出我们的第一步了。来看权重的迭代公式：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w}_{(t+1)}=\\boldsymbol{w}_{(t)}-\\eta \\frac{\\partial L(w)}{\\partial \\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "现在我们的偏导数部分已经计算出来了，就是我们用backward求解出的结果。而 $ \\eta $ 学习率，或者步长，是我们在迭代开始之前就认为设置好的，一般是0.01-0.5之间的某个小数，因此我们已经可以无障碍地使用代码实现权重的迭代了：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里，我们的数据是生成的随机数，为了让大家看出效果，所以设置步长为10，正常不会使用这么长的步长\n",
    "# 步长、学习率的英文是learning rate，所以常常简写为lr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(420)\n",
    "X = torch.rand((500, 20), dtype= torch.float32)*100\n",
    "y = torch.randint(low= 0, high= 3, size=(500,), dtype= torch.float32)\n",
    "in_features = X.shape[1]\n",
    "out_features = len(y.unique())\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, in_features= 10, out_features=2):\n",
    "    super().__init__()\n",
    "    self.linear_1 = nn.Linear(in_features, 13, bias= False)\n",
    "    self.linear_2 = nn.Linear(13, 8, bias = False)\n",
    "    self.out = nn.Linear(8, out_features, bias = False)\n",
    "\n",
    "  def forward(self, X):\n",
    "    z1 = self.linear_1(X)\n",
    "    sigma_1 = F.relu(z1)\n",
    "    z2 = self.linear_2(sigma_1)\n",
    "    sigma_2 = F.sigmoid(z2)\n",
    "    zhat = self.out(sigma_2)\n",
    "    return zhat\n",
    "\n",
    "torch.manual_seed(420)\n",
    "net = Model(in_features, out_features)\n",
    "zhat = net.forward(X)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(zhat, y.long())\n",
    "loss.backward()\n",
    "w = net.linear_1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3728e-01, -1.3441e-01,  2.1369e-01, -1.7753e-01, -6.7905e-02,\n",
       "         -1.5396e-01,  1.7325e-01,  8.4504e-02, -1.1130e-01, -1.7284e-01,\n",
       "         -1.2928e-01, -4.2774e-02, -1.1391e-01,  1.6372e-01, -9.3821e-02,\n",
       "         -1.4613e-01, -6.8671e-02, -2.1835e-01, -1.0836e-01, -1.2178e-01],\n",
       "        [-1.2292e-01,  1.0665e-01, -1.6468e-01, -2.0308e-01, -3.5176e-02,\n",
       "         -1.9426e-01, -2.2805e-01, -1.5618e-01, -3.2817e-01, -1.4099e-01,\n",
       "          3.6728e-02, -1.1321e-01, -8.2137e-02, -2.5312e-01, -3.1246e-01,\n",
       "         -1.8933e-01, -2.4277e-01, -4.0041e-02, -2.6232e-01,  1.2886e-02],\n",
       "        [-7.3430e-02, -3.6251e-02,  1.2690e-01, -3.6448e-02,  4.1469e-02,\n",
       "          1.4727e-01,  1.3623e-01, -1.7868e-01,  1.5008e-01,  1.9492e-01,\n",
       "          1.7009e-01, -2.6587e-01, -1.0714e-01, -1.7380e-01,  1.4086e-01,\n",
       "          1.4875e-01, -1.1610e-01,  2.7153e-01, -2.1605e-01, -5.7652e-03],\n",
       "        [ 2.8962e-01,  5.5489e-02,  8.9793e-02,  2.9763e-01, -8.9448e-02,\n",
       "          2.0657e-01, -1.8569e-02,  3.4607e-01, -1.3021e-01,  1.5119e-01,\n",
       "          2.1920e-01, -6.6122e-02, -1.2324e-01,  2.0248e-01, -8.9006e-02,\n",
       "          2.5089e-01,  9.7880e-02, -1.1030e-01,  2.5087e-01, -8.4947e-02],\n",
       "        [ 1.8127e-01,  2.6061e-01,  6.5753e-02,  2.3268e-01,  4.1831e-02,\n",
       "         -8.9380e-02, -9.6807e-02,  3.2999e-01,  1.5213e-01,  2.7888e-01,\n",
       "         -2.1895e-01,  1.9095e-01,  1.4215e-02, -1.1483e-01, -8.8083e-02,\n",
       "          2.6464e-01,  2.4740e-01, -6.6154e-02,  2.4714e-01,  8.8299e-02],\n",
       "        [ 4.2961e-02, -1.8638e-01, -9.9397e-02, -1.2088e-01, -1.1783e-01,\n",
       "         -1.4985e-01, -1.7261e-01,  2.5023e-02, -1.7564e-01, -1.6344e-01,\n",
       "          2.1170e-01,  1.8526e-01, -9.0934e-02, -6.7863e-02,  1.7450e-01,\n",
       "         -6.0034e-02, -2.8280e-02, -2.3743e-02, -1.9306e-01,  1.9985e-01],\n",
       "        [-4.1438e-03,  1.9206e-01,  1.9836e-01,  1.0056e-01,  1.2251e-01,\n",
       "         -1.3761e-01, -4.1686e-02, -1.8635e-01,  1.7146e-01, -4.4136e-01,\n",
       "          1.4277e-01, -2.0344e-01, -4.9978e-02,  1.0311e-01,  2.8124e-01,\n",
       "         -8.4192e-05,  1.2132e-01,  4.6479e-02, -3.0879e-01, -4.7474e-02],\n",
       "        [-2.0621e-01,  1.4347e-01, -2.9977e-01,  1.2644e-01,  7.1986e-02,\n",
       "         -3.8230e-02,  2.0139e-02, -7.4660e-02, -3.3019e-01, -2.3873e-02,\n",
       "          3.9792e-03, -9.1283e-02, -1.9801e-01, -1.7405e-01, -3.5226e-01,\n",
       "         -1.8233e-01, -3.2495e-02, -1.8831e-01, -1.3022e-01, -2.2954e-01],\n",
       "        [ 2.5535e-01,  1.2737e-01,  1.3140e-01, -1.2694e-01,  2.0116e-01,\n",
       "          1.5046e-01, -1.2868e-01,  8.5262e-02,  1.5964e-01,  2.9791e-01,\n",
       "         -1.0439e-01,  2.2681e-01,  1.0390e-01,  1.3868e-01, -5.6394e-02,\n",
       "          3.7180e-01, -6.2136e-02,  9.7962e-03,  3.0521e-01,  2.9950e-01],\n",
       "        [ 1.3852e-01,  1.3767e-01, -6.8379e-02,  1.6088e-01,  7.7797e-02,\n",
       "          1.5437e-01,  1.6502e-01,  2.6393e-01, -1.2685e-01,  1.4013e-01,\n",
       "         -1.5167e-01,  6.5657e-02, -1.5744e-01,  1.3041e-01, -1.4928e-01,\n",
       "         -1.9463e-01, -2.0437e-01,  1.8902e-02, -2.4257e-02, -3.8698e-02],\n",
       "        [ 1.5065e-01,  4.1561e-02, -1.6937e-01,  1.3888e-01, -1.7828e-01,\n",
       "          6.0200e-02, -3.8758e-02, -2.1206e-01, -1.5129e-01, -2.3346e-01,\n",
       "         -2.1464e-01, -9.6454e-02, -9.2178e-02,  1.4781e-01, -1.4655e-01,\n",
       "          5.2360e-02,  1.8667e-01,  1.3193e-01, -8.2965e-02, -2.3388e-01],\n",
       "        [ 1.1919e-01,  1.3481e-01,  1.9327e-02,  1.2611e-01,  8.0950e-03,\n",
       "          3.6548e-01,  2.2171e-01, -4.6017e-02,  3.4402e-01,  2.8776e-02,\n",
       "          1.9939e-01,  1.5492e-01,  2.3045e-01,  3.8289e-01,  5.1445e-03,\n",
       "          2.9804e-01,  1.2940e-01,  1.4798e-01, -7.3968e-02,  1.8330e-01],\n",
       "        [-1.2511e-01, -7.2119e-02, -2.1989e-01, -1.5447e-01,  1.8175e-01,\n",
       "         -7.2319e-02, -8.3032e-02,  2.0897e-01, -4.7049e-02, -8.0213e-03,\n",
       "         -8.4303e-03, -1.0178e-01, -8.2231e-02, -1.8899e-01,  1.6787e-01,\n",
       "         -1.7747e-01, -1.0798e-01, -3.5530e-02, -1.7967e-01,  1.7934e-02]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 10\n",
    "dw = net.linear_1.weight.grad\n",
    "w -= lr*dw # type: ignore\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 第一步到第二步：动量法Momentum\n",
    "\n",
    "普通梯度下降就是在重复正向传播，计算梯度，更新权重的过程，但这个过程往往非常漫长。如大家所见，步长设置为0.001时，我们看不到 w 任何变化，只有当步长设置得非常巨大，我们才能够看到一些改变，但在之前的过程我们说过，巨大的步长可能会让我们跳过真正的最小值，所以我们无法将步长设置得非常大，无论如何，梯度下降都是一个非常慢的过程。在这个基础上，我们提出了加速迭代的数个方法，其中一个很关键的方法，就是使用动量Momentum。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们说过，在梯度下降过程中，起始点是一个“盲人”，它看不到也听不到全局，所以我们每移动一次都要重新计算方向与距离，并且每次只能走一小步。但不只限于次，起始点不仅看不到前面的路，也无法从过去走过的路中学习。\n",
    "\n",
    "想象一下，我们被蒙上眼睛，由另一个人喊口号来给我们方向让我们移动，假设喊口号的人一直喊：“向前，向前，向前”。因为我们看不见，在最初的三四次，我们可能只会向前走一小步，但如果他一直让我们向前，我们就会失去耐心，转而向前走一大步，因为我们可以预测：前面很长一段路大概都是需要向前的。对梯度下降来说，也是同样的道理——如果在很长一段时间内，起始点一直向着相似的方向移动，那按照步长一小步一小步地磨着向前走是没有意义的，既浪费计算资源又浪费时间，此时就应该大胆地按照这个方向走一大步。相对的，如果我们很长时间都走向同一个方向，突然让我们转向，那我们转向的第一步就应该非常谨慎，走一小步。\n",
    "\n",
    "不难发现，真正高效的方法是：在历史方向与现有方向相同的情况下，迈出大步子，在历史方向与现有方向相反的情况下，迈出小步子。那要怎么才能让起始点了解过去的方向呢？**我们让上一步的梯度向量（的反方向）与现在这一点的梯度向量（的反方向）以加权的方式求和，求解出受到上一步大小和方向影响的真实下降方向，再让坐标点向真实下降方向移动**。在坐标轴上可以表示为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，对于上一步的梯度向量加上的权重被称为动量参数（也叫做衰减力度，通常使用 $ \\gamma $ 进行表示），对这一点的梯度向量加上权重就是步长（依然为 $ \\eta $）,真实移动的向量为 $ v $，被称为动量。将上述过程使用公式表示，则有：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{(t)} & =\\gamma v_{(t-1)}-\\eta \\frac{L}{\\partial \\boldsymbol{w}} \\\\\n",
    "\\boldsymbol{w}_{(t+1)} & =\\boldsymbol{w}_{(t)}+v_{(t)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "在第一步中，没有历史梯度方向，因此第一步的真实方向就是起始点梯度的反方向， $ v_{0} = 0$ 。其中$ v_{(t-1)}$ 代表了之前所有步骤所累计的动量和（其实也就代表着上一步的真实方向）。在这种情况下，梯度下降的方向有了惯性，受到历史累积动量的影响，当新坐标点的梯度反方向与历史累积动量的方向一致时，历史累积动量会加大实际方向的步子，相反，当新坐标点的梯度方向与历史累积动量的方向不一致时，历史累积动量会减小实际方向的步子。\n",
    "\n",
    "我们可以很容易地在PyTorch中实现动量法：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v(t) = gamma * v(t-1) - lr * dw\n",
    "# w(t+1) = w(t) + v(t)\n",
    "\n",
    "lr = 0.1\n",
    "gamma = 0.9\n",
    "dw = net.linear_1.weight.grad.data\n",
    "w = net.linear_1.weight.data\n",
    "\n",
    "# t = 1,走出第一步，进行首次迭代的时候，需要一个v0\n",
    "v = torch.zeros(dw.shape[0], dw.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1373, -0.1344,  0.2137, -0.1775, -0.0679, -0.1540,  0.1733,  0.0845,\n",
       "         -0.1113, -0.1728, -0.1293, -0.0428, -0.1139,  0.1637, -0.0938, -0.1461,\n",
       "         -0.0687, -0.2184, -0.1084, -0.1218],\n",
       "        [-0.1246,  0.1059, -0.1666, -0.2038, -0.0364, -0.1953, -0.2293, -0.1573,\n",
       "         -0.3303, -0.1426,  0.0353, -0.1147, -0.0843, -0.2537, -0.3138, -0.1909,\n",
       "         -0.2452, -0.0418, -0.2630,  0.0121],\n",
       "        [-0.0722, -0.0370,  0.1267, -0.0371,  0.0412,  0.1473,  0.1354, -0.1791,\n",
       "          0.1508,  0.1950,  0.1705, -0.2665, -0.1067, -0.1734,  0.1413,  0.1480,\n",
       "         -0.1151,  0.2725, -0.2161, -0.0056],\n",
       "        [ 0.2907,  0.0564,  0.0904,  0.2990, -0.0885,  0.2081, -0.0177,  0.3475,\n",
       "         -0.1296,  0.1525,  0.2200, -0.0655, -0.1232,  0.2037, -0.0881,  0.2521,\n",
       "          0.0986, -0.1092,  0.2526, -0.0839],\n",
       "        [ 0.1828,  0.2618,  0.0668,  0.2342,  0.0424, -0.0886, -0.0956,  0.3314,\n",
       "          0.1528,  0.2811, -0.2191,  0.1931,  0.0139, -0.1140, -0.0872,  0.2661,\n",
       "          0.2485, -0.0654,  0.2484,  0.0895],\n",
       "        [ 0.0430, -0.1865, -0.0994, -0.1208, -0.1178, -0.1500, -0.1726,  0.0249,\n",
       "         -0.1756, -0.1634,  0.2117,  0.1853, -0.0910, -0.0678,  0.1745, -0.0601,\n",
       "         -0.0283, -0.0239, -0.1932,  0.1999],\n",
       "        [-0.0040,  0.1918,  0.1985,  0.1006,  0.1238, -0.1390, -0.0422, -0.1860,\n",
       "          0.1737, -0.4447,  0.1443, -0.2057, -0.0487,  0.1024,  0.2822, -0.0015,\n",
       "          0.1215,  0.0466, -0.3103, -0.0488],\n",
       "        [-0.2091,  0.1427, -0.3007,  0.1258,  0.0709, -0.0390,  0.0196, -0.0757,\n",
       "         -0.3333, -0.0246,  0.0027, -0.0925, -0.1998, -0.1757, -0.3541, -0.1831,\n",
       "         -0.0346, -0.1903, -0.1318, -0.2299],\n",
       "        [ 0.2563,  0.1286,  0.1330, -0.1260,  0.2018,  0.1523, -0.1281,  0.0860,\n",
       "          0.1591,  0.3000, -0.1046,  0.2292,  0.1039,  0.1402, -0.0556,  0.3738,\n",
       "         -0.0608,  0.0102,  0.3073,  0.3023],\n",
       "        [ 0.1384,  0.1376, -0.0688,  0.1617,  0.0785,  0.1552,  0.1651,  0.2649,\n",
       "         -0.1264,  0.1404, -0.1513,  0.0661, -0.1574,  0.1307, -0.1490, -0.1947,\n",
       "         -0.2044,  0.0193, -0.0243, -0.0387],\n",
       "        [ 0.1502,  0.0414, -0.1696,  0.1386, -0.1784,  0.0602, -0.0387, -0.2121,\n",
       "         -0.1514, -0.2337, -0.2149, -0.0965, -0.0923,  0.1477, -0.1467,  0.0523,\n",
       "          0.1864,  0.1316, -0.0832, -0.2341],\n",
       "        [ 0.1206,  0.1363,  0.0199,  0.1270,  0.0093,  0.3671,  0.2227, -0.0443,\n",
       "          0.3462,  0.0305,  0.2003,  0.1560,  0.2319,  0.3847,  0.0064,  0.2991,\n",
       "          0.1310,  0.1494, -0.0726,  0.1841],\n",
       "        [-0.1251, -0.0722, -0.2199, -0.1545,  0.1816, -0.0724, -0.0831,  0.2089,\n",
       "         -0.0471, -0.0082, -0.0085, -0.1018, -0.0823, -0.1890,  0.1678, -0.1775,\n",
       "         -0.1080, -0.0357, -0.1797,  0.0179]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = gamma*v - lr*dw\n",
    "w = w + v\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 torch.optim实现带动量的梯度下降\n",
    "\n",
    "在PyTorch库的架构中，拥有专门实现优化算法的模块torch.optim。我们在之前的课程中所说的迭代流程，都可以通过torch.optim模块来简单地实现。\n",
    "\n",
    "接下来，我们就基于之前定义的类Model来实现梯度下降的一轮迭代：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "# 确定数据，超参数的确定(lr, gamma)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 确定数据\n",
    "torch.manual_seed(420)\n",
    "X = torch.rand(500, 20, dtype= torch.float32)*100\n",
    "y = torch.randint(low= 0, high= 3, size= (500,), dtype= torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "lr = 0.1\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络的架构\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, in_features= 10, out_features=2):\n",
    "    super().__init__()\n",
    "    self.linear_1 = nn.Linear(in_features, 13, bias= True)\n",
    "    self.linear_2 = nn.Linear(13, 8, bias = True)\n",
    "    self.out = nn.Linear(8, out_features, bias = True)\n",
    "\n",
    "  def forward(self, X):\n",
    "    z1 = self.linear_1(X)\n",
    "    sigma_1 = F.relu(z1)\n",
    "    z2 = self.linear_2(sigma_1)\n",
    "    sigma_2 = F.sigmoid(z2)\n",
    "    zhat = self.out(sigma_2)\n",
    "    return zhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = X.shape[1]\n",
    "output = len(y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化神经网络\n",
    "torch.manual_seed(420)\n",
    "net = Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化算法\n",
    "opt = optim.SGD(net.parameters()\n",
    "         ,lr = lr\n",
    "         ,momentum = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向前传播\n",
    "# 本轮向前传播的损失函数值\n",
    "# 反向传播 - 得到梯度\n",
    "# 更新权重(和动量)\n",
    "# 清空梯度- 清除原来计算出来的，基于上一个点的坐标计算的梯度\n",
    "\n",
    "zhat = net.forward(X)\n",
    "loss = criterion(zhat, y.long())\n",
    "loss.backward()\n",
    "opt.step()    # 步子，走一步，更新权重W，更新动量V\n",
    "opt.zero_grad()\n",
    "\n",
    "print(loss)\n",
    "print(net.linear_1.weight.data[0][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
